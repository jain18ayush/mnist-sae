{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch \n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "path = '/Volumes/Sid_Drive/mnist/'\n",
    "\n",
    "if os.path.exists(path):\n",
    "    prefix = path\n",
    "else:\n",
    "    prefix = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/Sid_Drive/mnist/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import os\n",
    "\n",
    "class CrossDatasetAnalyzer:\n",
    "    def __init__(self, dataset_names, max_depth=9, prefix=''):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            prefix: Path prefix for loading files\n",
    "            dataset_names: List of dataset names to analyze\n",
    "            max_depth: Maximum depth to analyze\n",
    "        \"\"\"\n",
    "        self.prefix = prefix\n",
    "        self.dataset_names = dataset_names\n",
    "        self.max_depth = max_depth\n",
    "        os.makedirs('plots/hypothesis', exist_ok=True)\n",
    "    \n",
    "    def load_depth_embeddings(self, depth, dataset_name):\n",
    "        \"\"\"Load embeddings for a specific depth and dataset\"\"\"\n",
    "        path = f'{self.prefix}embeddings/mnist_encoder_{dataset_name}_depth_{depth}.pth'\n",
    "        return torch.load(path)\n",
    "\n",
    "    def analyze_activation_patterns(self, activations):\n",
    "        \"\"\"Analyze activation patterns\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # 1. Activation Statistics\n",
    "        metrics['mean_activation'] = torch.mean(activations).item()\n",
    "        metrics['activation_std'] = torch.std(activations).item()\n",
    "        metrics['sparsity'] = (activations == 0).float().mean().item()\n",
    "        \n",
    "        # 2. Active Feature Count\n",
    "        threshold = activations.mean() + activations.std()\n",
    "        active_features = (activations > threshold).sum(dim=1)\n",
    "        metrics['avg_active_features'] = active_features.float().mean().item()\n",
    "        \n",
    "        # 3. Feature Utilization\n",
    "        feature_usage = (activations > threshold).float().mean(dim=0)\n",
    "        metrics['feature_utilization'] = feature_usage.mean().item()\n",
    "        metrics['feature_utilization_std'] = feature_usage.std().item()\n",
    "        \n",
    "        # 4. Activation Distribution\n",
    "        normalized = torch.nn.functional.softmax(activations, dim=1)\n",
    "        activation_entropy = entropy(normalized.numpy(), axis=1)\n",
    "        metrics['activation_entropy'] = np.mean(activation_entropy)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def analyze_feature_overlap(self, activations_by_depth):\n",
    "        \"\"\"Measure how much features overlap between consecutive depths\"\"\"\n",
    "        overlaps = []\n",
    "        for i in range(len(activations_by_depth)-1):\n",
    "            current = activations_by_depth[i]\n",
    "            next_depth = activations_by_depth[i+1]\n",
    "            \n",
    "            # Compute cosine similarity between features at adjacent depths\n",
    "            similarity_matrix = F.cosine_similarity(current.unsqueeze(1), \n",
    "                                                  next_depth.unsqueeze(0))\n",
    "            \n",
    "            # Track maximum similarities for each feature\n",
    "            max_similarities = similarity_matrix.max(dim=1)[0]\n",
    "            overlaps.append({\n",
    "                'depth': i + 1,\n",
    "                'mean_overlap': max_similarities.mean().item(),\n",
    "                'std_overlap': max_similarities.std().item()\n",
    "            })\n",
    "            \n",
    "        return overlaps\n",
    "\n",
    "    def measure_feature_specificity(self, activations):\n",
    "        \"\"\"Measure how specifically features respond to inputs\"\"\"\n",
    "        # Calculate activation distributions\n",
    "        mean_activations = activations.mean(dim=0)\n",
    "        std_activations = activations.std(dim=0)\n",
    "        \n",
    "        # Calculate peakedness (kurtosis) of activation distributions\n",
    "        kurtosis = ((activations - mean_activations)**4).mean(dim=0) / (std_activations**4 + 1e-10)\n",
    "        \n",
    "        return {\n",
    "            'mean_kurtosis': kurtosis.mean().item(),\n",
    "            'std_kurtosis': kurtosis.std().item()\n",
    "        }\n",
    "\n",
    "    def analyze_feature_hierarchy(self, activations):\n",
    "        \"\"\"Analyze hierarchical relationships between features with proper handling of edge cases\"\"\"\n",
    "        # Add small epsilon to avoid zero variance\n",
    "        eps = 1e-8\n",
    "        activations = activations + eps\n",
    "        \n",
    "        # Remove features with zero or near-zero variance\n",
    "        variances = torch.var(activations, dim=0)\n",
    "        valid_features = variances > eps\n",
    "        \n",
    "        if not torch.any(valid_features):\n",
    "            return {\n",
    "                'n_clusters': 1,\n",
    "                'avg_cluster_size': activations.shape[1],\n",
    "                'linkage_matrix': None,\n",
    "                'error': 'No valid features found'\n",
    "            }\n",
    "        \n",
    "        # Filter activations to only include valid features\n",
    "        filtered_activations = activations[:, valid_features]\n",
    "        \n",
    "        try:\n",
    "            # Compute feature similarity matrix\n",
    "            feature_similarities = torch.corrcoef(filtered_activations.T)\n",
    "            \n",
    "            # Convert to numpy and handle any remaining NaN values\n",
    "            feature_similarities_np = feature_similarities.numpy()\n",
    "            feature_similarities_np = np.nan_to_num(feature_similarities_np)\n",
    "            \n",
    "            # Ensure the matrix is symmetric and contains only finite values\n",
    "            np.fill_diagonal(feature_similarities_np, 1.0)\n",
    "            feature_similarities_np = (feature_similarities_np + feature_similarities_np.T) / 2\n",
    "            \n",
    "            # Convert similarities to distances\n",
    "            distances = 1 - np.abs(feature_similarities_np)\n",
    "            \n",
    "            # Perform hierarchical clustering\n",
    "            linkage_matrix = linkage(distances, method='ward')\n",
    "            \n",
    "            # Analyze cluster structure\n",
    "            n_clusters = len(np.unique(linkage_matrix[:, -1]))\n",
    "            avg_cluster_size = filtered_activations.shape[1] / n_clusters\n",
    "            \n",
    "            return {\n",
    "                'n_clusters': n_clusters,\n",
    "                'avg_cluster_size': avg_cluster_size,\n",
    "                'linkage_matrix': linkage_matrix,\n",
    "                'error': None\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in hierarchical clustering: {str(e)}\")\n",
    "            return {\n",
    "                'n_clusters': 1,\n",
    "                'avg_cluster_size': activations.shape[1],\n",
    "                'linkage_matrix': None,\n",
    "                'error': str(e)\n",
    "            }\n",
    "        \n",
    "    def analyze_depth_patterns(self, activations):\n",
    "        \"\"\"Analyze activation patterns at a specific depth\"\"\"\n",
    "        # Count unique activation patterns\n",
    "        discretized = (activations > activations.mean()).float()\n",
    "        unique_patterns = torch.unique(discretized, dim=0).shape[0]\n",
    "        \n",
    "        # Measure average activation sparsity\n",
    "        sparsity = (activations == 0).float().mean()\n",
    "        \n",
    "        # Calculate activation entropy\n",
    "        normalized = F.softmax(activations, dim=1)\n",
    "        entropy = -(normalized * torch.log(normalized + 1e-10)).sum(1).mean()\n",
    "        \n",
    "        return {\n",
    "            'unique_patterns': unique_patterns,\n",
    "            'sparsity': sparsity.item(),\n",
    "            'entropy': entropy.item()\n",
    "        }\n",
    "\n",
    "    def analyze_feature_composition(self, prev_activations, curr_activations):\n",
    "        \"\"\"Analyze how features are composed across depths\"\"\"\n",
    "        # Fit linear regression to see how current features are composed\n",
    "        regression = torch.linalg.lstsq(prev_activations, curr_activations)\n",
    "        \n",
    "        # Analyze composition weights\n",
    "        weight_sparsity = (regression.solution == 0).float().mean()\n",
    "        weight_distribution = regression.solution.std(dim=0)\n",
    "        \n",
    "        return {\n",
    "            'weight_sparsity': weight_sparsity.item(),\n",
    "            'weight_distribution': weight_distribution.mean().item()\n",
    "        }\n",
    "\n",
    "    def compare_datasets(self):\n",
    "        \"\"\"Compare activation patterns across datasets and depths\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for dataset_name in tqdm(self.dataset_names, desc=\"Processing datasets\"):\n",
    "            dataset_results = {\n",
    "                'basic_metrics': [],\n",
    "                'feature_specificity': [],\n",
    "                'hierarchy': [],\n",
    "                'patterns': [],\n",
    "                'compositions': []\n",
    "            }\n",
    "            activations_by_depth = []\n",
    "            \n",
    "            for depth in tqdm(range(1, self.max_depth + 1), desc=f\"Processing depth for {dataset_name}\"):\n",
    "                try:\n",
    "                    # Load embeddings for this depth\n",
    "                    activations = self.load_depth_embeddings(depth, dataset_name)\n",
    "                    activations_by_depth.append(activations)\n",
    "                    \n",
    "                    # Basic metrics\n",
    "                    basic_metrics = self.analyze_activation_patterns(activations)\n",
    "                    dataset_results['basic_metrics'].append(basic_metrics)\n",
    "                    \n",
    "                    # Feature specificity\n",
    "                    specificity = self.measure_feature_specificity(activations)\n",
    "                    dataset_results['feature_specificity'].append(specificity)\n",
    "                    \n",
    "                    # Hierarchy analysis\n",
    "                    hierarchy = self.analyze_feature_hierarchy(activations)\n",
    "                    dataset_results['hierarchy'].append(hierarchy)\n",
    "                    \n",
    "                    # Pattern analysis\n",
    "                    patterns = self.analyze_depth_patterns(activations)\n",
    "                    dataset_results['patterns'].append(patterns)\n",
    "                    \n",
    "                    # Feature composition (skip first depth)\n",
    "                    if depth > 1:\n",
    "                        composition = self.analyze_feature_composition(\n",
    "                            activations_by_depth[-2], activations)\n",
    "                        dataset_results['compositions'].append(composition)\n",
    "                    \n",
    "                except FileNotFoundError:\n",
    "                    print(f\"No embeddings found for {dataset_name} at depth {depth}\")\n",
    "                    break\n",
    "            \n",
    "            # Analyze feature overlap across all depths\n",
    "            if len(activations_by_depth) > 1:\n",
    "                dataset_results['overlaps'] = self.analyze_feature_overlap(activations_by_depth)\n",
    "            \n",
    "            results[dataset_name] = dataset_results\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def plot_metrics_across_depths(self, results):\n",
    "        \"\"\"Plot comprehensive metrics across depths for each dataset\"\"\"\n",
    "        # Set the style\n",
    "        plt.style.use('seaborn')\n",
    "        \n",
    "        for dataset_name, dataset_results in results.items():\n",
    "            # Create directory for this dataset's plots\n",
    "            save_dir = f\"plots/hypothesis/{dataset_name}\"\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            \n",
    "            # 1. Basic Activation Metrics\n",
    "            if dataset_results['basic_metrics']:\n",
    "                metrics = dataset_results['basic_metrics'][0].keys()\n",
    "                for metric in metrics:\n",
    "                    plt.figure(figsize=(10, 6))\n",
    "                    values = [m[metric] for m in dataset_results['basic_metrics']]\n",
    "                    plt.plot(range(1, len(values) + 1), values, 'o-', linewidth=2, markersize=8)\n",
    "                    plt.xlabel('Depth', fontsize=12)\n",
    "                    plt.ylabel(metric.replace('_', ' ').title(), fontsize=12)\n",
    "                    plt.title(f'{metric.replace(\"_\", \" \").title()} vs Depth for {dataset_name}', fontsize=14)\n",
    "                    plt.grid(True, alpha=0.3)\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(f\"{save_dir}/{metric}.png\", dpi=300, bbox_inches='tight')\n",
    "                    plt.close()\n",
    "            \n",
    "            # 2. Feature Specificity Plot\n",
    "            if dataset_results['feature_specificity']:\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                mean_kurtosis = [m['mean_kurtosis'] for m in dataset_results['feature_specificity']]\n",
    "                std_kurtosis = [m['std_kurtosis'] for m in dataset_results['feature_specificity']]\n",
    "                depths = range(1, len(mean_kurtosis) + 1)\n",
    "                \n",
    "                plt.plot(depths, mean_kurtosis, 'b-o', label='Mean Kurtosis', linewidth=2, markersize=8)\n",
    "                plt.fill_between(depths, \n",
    "                            [m - s for m, s in zip(mean_kurtosis, std_kurtosis)],\n",
    "                            [m + s for m, s in zip(mean_kurtosis, std_kurtosis)],\n",
    "                            alpha=0.2, color='blue')\n",
    "                plt.xlabel('Depth', fontsize=12)\n",
    "                plt.ylabel('Feature Specificity (Kurtosis)', fontsize=12)\n",
    "                plt.title(f'Feature Specificity vs Depth for {dataset_name}', fontsize=14)\n",
    "                plt.legend(fontsize=10)\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"{save_dir}/feature_specificity.png\", dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "            \n",
    "            # 3. Hierarchical Structure Plot\n",
    "            if dataset_results['hierarchy']:\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                n_clusters = []\n",
    "                avg_sizes = []\n",
    "                depths = []\n",
    "                \n",
    "                for i, h in enumerate(dataset_results['hierarchy']):\n",
    "                    if h['error'] is None:  # Only include results without errors\n",
    "                        n_clusters.append(h['n_clusters'])\n",
    "                        avg_sizes.append(h['avg_cluster_size'])\n",
    "                        depths.append(i + 1)\n",
    "                \n",
    "                if depths:  # Only create plot if we have valid data\n",
    "                    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "                    \n",
    "                    # Number of clusters\n",
    "                    ax1.plot(depths, n_clusters, 'r-o', linewidth=2, markersize=8)\n",
    "                    ax1.set_xlabel('Depth', fontsize=12)\n",
    "                    ax1.set_ylabel('Number of Clusters', fontsize=12)\n",
    "                    ax1.set_title('Cluster Count vs Depth', fontsize=14)\n",
    "                    ax1.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    # Average cluster size\n",
    "                    ax2.plot(depths, avg_sizes, 'g-o', linewidth=2, markersize=8)\n",
    "                    ax2.set_xlabel('Depth', fontsize=12)\n",
    "                    ax2.set_ylabel('Average Cluster Size', fontsize=12)\n",
    "                    ax2.set_title('Cluster Size vs Depth', fontsize=14)\n",
    "                    ax2.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    plt.suptitle(f'Hierarchical Structure Analysis for {dataset_name}', fontsize=16)\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(f\"{save_dir}/hierarchy_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "                    plt.close()\n",
    "            \n",
    "            # 4. Pattern Analysis Plot\n",
    "            if dataset_results['patterns']:\n",
    "                plt.figure(figsize=(15, 5))\n",
    "                fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "                depths = range(1, len(dataset_results['patterns']) + 1)\n",
    "                \n",
    "                # Unique patterns\n",
    "                unique_patterns = [p['unique_patterns'] for p in dataset_results['patterns']]\n",
    "                ax1.plot(depths, unique_patterns, 'b-o', linewidth=2, markersize=8)\n",
    "                ax1.set_xlabel('Depth', fontsize=12)\n",
    "                ax1.set_ylabel('Unique Patterns', fontsize=12)\n",
    "                ax1.set_title('Pattern Diversity', fontsize=14)\n",
    "                ax1.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Sparsity\n",
    "                sparsity = [p['sparsity'] for p in dataset_results['patterns']]\n",
    "                ax2.plot(depths, sparsity, 'r-o', linewidth=2, markersize=8)\n",
    "                ax2.set_xlabel('Depth', fontsize=12)\n",
    "                ax2.set_ylabel('Sparsity', fontsize=12)\n",
    "                ax2.set_title('Activation Sparsity', fontsize=14)\n",
    "                ax2.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Entropy\n",
    "                entropy = [p['entropy'] for p in dataset_results['patterns']]\n",
    "                ax3.plot(depths, entropy, 'g-o', linewidth=2, markersize=8)\n",
    "                ax3.set_xlabel('Depth', fontsize=12)\n",
    "                ax3.set_ylabel('Entropy', fontsize=12)\n",
    "                ax3.set_title('Activation Entropy', fontsize=14)\n",
    "                ax3.grid(True, alpha=0.3)\n",
    "                \n",
    "                plt.suptitle(f'Activation Pattern Analysis for {dataset_name}', fontsize=16)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"{save_dir}/pattern_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "            \n",
    "            # 5. Feature Overlap Plot\n",
    "            if 'overlaps' in dataset_results and dataset_results['overlaps']:\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                mean_overlaps = [o['mean_overlap'] for o in dataset_results['overlaps']]\n",
    "                std_overlaps = [o['std_overlap'] for o in dataset_results['overlaps']]\n",
    "                depths = range(1, len(mean_overlaps) + 1)\n",
    "                \n",
    "                plt.plot(depths, mean_overlaps, 'p-', label='Mean Overlap', linewidth=2, markersize=8)\n",
    "                plt.fill_between(depths,\n",
    "                            [m - s for m, s in zip(mean_overlaps, std_overlaps)],\n",
    "                            [m + s for m, s in zip(mean_overlaps, std_overlaps)],\n",
    "                            alpha=0.2)\n",
    "                plt.xlabel('Depth', fontsize=12)\n",
    "                plt.ylabel('Feature Overlap', fontsize=12)\n",
    "                plt.title(f'Feature Overlap Analysis for {dataset_name}', fontsize=14)\n",
    "                plt.legend(fontsize=10)\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"{save_dir}/feature_overlap.png\", dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "            \n",
    "            # 6. Feature Composition Analysis\n",
    "            if dataset_results['compositions']:\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                sparsity = [c['weight_sparsity'] for c in dataset_results['compositions']]\n",
    "                distribution = [c['weight_distribution'] for c in dataset_results['compositions']]\n",
    "                depths = range(2, len(sparsity) + 2)  # Start from depth 2\n",
    "                \n",
    "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "                \n",
    "                ax1.plot(depths, sparsity, 'm-o', linewidth=2, markersize=8)\n",
    "                ax1.set_xlabel('Depth', fontsize=12)\n",
    "                ax1.set_ylabel('Weight Sparsity', fontsize=12)\n",
    "                ax1.set_title('Composition Sparsity', fontsize=14)\n",
    "                ax1.grid(True, alpha=0.3)\n",
    "                \n",
    "                ax2.plot(depths, distribution, 'c-o', linewidth=2, markersize=8)\n",
    "                ax2.set_xlabel('Depth', fontsize=12)\n",
    "                ax2.set_ylabel('Weight Distribution', fontsize=12)\n",
    "                ax2.set_title('Composition Distribution', fontsize=14)\n",
    "                ax2.grid(True, alpha=0.3)\n",
    "                \n",
    "                plt.suptitle(f'Feature Composition Analysis for {dataset_name}', fontsize=16)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"{save_dir}/feature_composition.png\", dpi=300, bbox_inches='tight')\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = CrossDatasetAnalyzer(['MNIST', 'CIFAR100', 'EMNIST_letter', 'EMNIST'], max_depth=9, prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datasets:   0%|          | 0/4 [00:00<?, ?it/s]/var/folders/bl/kzfk5ts90gj98y9jcv49ynmc0000gn/T/ipykernel_21233/1822133751.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(path)\n",
      "/var/folders/bl/kzfk5ts90gj98y9jcv49ynmc0000gn/T/ipykernel_21233/1822133751.py:128: ClusterWarning: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "  linkage_matrix = linkage(distances, method='ward')\n",
      "Processing depth for MNIST: 100%|██████████| 9/9 [10:12<00:00, 68.01s/it]\n"
     ]
    }
   ],
   "source": [
    "results = analyzer.compare_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.plot_metrics_across_depths(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
