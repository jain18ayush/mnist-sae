{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x11619aed0>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MNISTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNISTModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)  # Input layer (28x28 pixels)\n",
        "        self.fc2 = nn.Linear(128, 64)       # Hidden layer\n",
        "        self.fc3 = nn.Linear(64, 10)        # Output layer (10 classes)\n",
        "\n",
        "        # Initialize a cache to store lists of activations\n",
        "        self.activation_cache = {\n",
        "            'fc1': [],\n",
        "            'fc2': [],\n",
        "            'fc3': []\n",
        "        }\n",
        "\n",
        "    def forward(self, x, cache_activations=False):\n",
        "        x = x.view(-1, 28 * 28)  # Flatten the input image\n",
        "        \n",
        "        # Pass through fc1 and cache activations if needed\n",
        "        fc1_out = torch.relu(self.fc1(x))\n",
        "        if cache_activations:\n",
        "            self.activation_cache['fc1'].append(fc1_out.detach().clone())  # Append fc1 activations\n",
        "        \n",
        "        # Pass through fc2 and cache activations if needed\n",
        "        fc2_out = torch.relu(self.fc2(fc1_out))\n",
        "        if cache_activations:\n",
        "            self.activation_cache['fc2'].append(fc2_out.detach().clone())  # Append fc2 activations\n",
        "        \n",
        "        # Pass through fc3 and cache activations if needed\n",
        "        fc3_out = self.fc3(fc2_out)\n",
        "        if cache_activations:\n",
        "            self.activation_cache['fc3'].append(fc3_out.detach().clone())  # Append fc3 activations\n",
        "\n",
        "        return fc3_out\n",
        "\n",
        "    # Method to retrieve cached activations for a specified layer\n",
        "    def get_cached_activations(self, layer_name):\n",
        "        return torch.cat(self.activation_cache[layer_name]) if layer_name in self.activation_cache else None\n",
        "\n",
        "    # Method to clear the cache\n",
        "    def clear_cache(self):\n",
        "        self.activation_cache = {\n",
        "            'fc1': [],\n",
        "            'fc2': [],\n",
        "            'fc3': []\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IkB3CSGham1e",
        "outputId": "eeb79734-1272-4180-e8f2-a56ec1ccb09a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Loss: 0.2645\n",
            "Epoch [2/5], Loss: 0.1110\n",
            "Epoch [3/5], Loss: 0.0776\n",
            "Epoch [4/5], Loss: 0.0612\n",
            "Epoch [5/5], Loss: 0.0484\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "epochs = 5\n",
        "\n",
        "# MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # Mean and std deviation of MNIST dataset\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "MNIST_test_dataset = test_dataset #* Test dataset for mnist class \n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize model, loss function and optimizer\n",
        "model = MNISTModel()\n",
        "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "all_activations = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()  # Zero out previous gradients\n",
        "        outputs = model(images)  # Get output and activations from fc2\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Gradient descent\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Store activations for the sparse autoencoder\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'models/mnist_model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 97.58%\n"
          ]
        }
      ],
      "source": [
        "#* Test Accuracy Loop \n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Test accuracy: {100 * correct / total}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cache Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#* Test Accuracy Loop \n",
        "model.clear_cache()\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images, cache_activations=True)\n",
        "        _, predicted = torch.max(outputs.data, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([10000, 128]), torch.Size([10000, 64]), torch.Size([10000, 10]))"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.get_cached_activations('fc1').shape, model.get_cached_activations('fc2').shape, model.get_cached_activations('fc3').shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([10000])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_loader.dataset.targets.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Maximally Activating Image Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "class AnalysisMNISTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AnalysisMNISTModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        fc1_out = torch.relu(self.fc1(x))\n",
        "        fc2_out = torch.relu(self.fc2(fc1_out))\n",
        "        fc3_out = self.fc3(fc2_out)\n",
        "        return fc3_out, fc1_out, fc2_out\n",
        "\n",
        "def gaussian_blur(img, kernel_size=3, sigma=1.0):\n",
        "    # Ensure the image has a batch dimension\n",
        "    if img.dim() == 3:\n",
        "        img = img.unsqueeze(0)\n",
        "    \n",
        "    # Create the Gaussian kernel\n",
        "    kernel = torch.tensor([\n",
        "        [1, 2, 1],\n",
        "        [2, 4, 2],\n",
        "        [1, 2, 1]\n",
        "    ], dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "    kernel = kernel / kernel.sum()\n",
        "    \n",
        "    # Apply the convolution\n",
        "    padding = (kernel_size - 1) // 2\n",
        "    blurred = F.conv2d(img, kernel, padding=padding, groups=img.shape[1])\n",
        "    \n",
        "    return blurred.squeeze(0)  # Remove the batch dimension\n",
        "\n",
        "def activation_maximization(model, layer_index, neuron_index, num_iterations=500, learning_rate=0.1):\n",
        "    model.eval()\n",
        "    \n",
        "    # Create a random input image\n",
        "    input_image = torch.randn(1, 1, 28, 28, requires_grad=True)\n",
        "    \n",
        "    # Define the optimizer\n",
        "    optimizer = torch.optim.Adam([input_image], lr=learning_rate)\n",
        "    \n",
        "    # Define transforms for regularization\n",
        "    jitter = transforms.Compose([\n",
        "        transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=15),\n",
        "    ])\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Apply jitter transformation\n",
        "        jittered_image = jitter(input_image)\n",
        "        \n",
        "        # Apply custom Gaussian blur\n",
        "        blurred_image = gaussian_blur(jittered_image, kernel_size=3, sigma=0.5)\n",
        "        \n",
        "        # Forward pass\n",
        "        _, fc1_out, fc2_out = model(blurred_image)\n",
        "        \n",
        "        # Get the activation of the specified neuron\n",
        "        if layer_index == 0:\n",
        "            target_activation = fc1_out[0, neuron_index]\n",
        "        elif layer_index == 1:\n",
        "            target_activation = fc2_out[0, neuron_index]\n",
        "        else:\n",
        "            raise ValueError(\"Invalid layer index\")\n",
        "        \n",
        "        # Compute the loss (negative activation to maximize it)\n",
        "        loss = -target_activation\n",
        "        \n",
        "        # Add L2 regularization\n",
        "        l2_reg = 1e-3 * torch.sum(input_image**2)\n",
        "        loss += l2_reg\n",
        "        \n",
        "        # Add total variation regularization\n",
        "        tv_reg = 1e-4 * (torch.sum(torch.abs(input_image[:,:,:-1] - input_image[:,:,1:])) + \n",
        "                         torch.sum(torch.abs(input_image[:,:-1,:] - input_image[:,1:,:])))\n",
        "        loss += tv_reg\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update the input image\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Clip the image values to be between 0 and 1\n",
        "        with torch.no_grad():\n",
        "            input_image.clamp_(0, 1)\n",
        "        \n",
        "        # Periodically apply sharpening\n",
        "        if i % 50 == 0:\n",
        "            input_image.data = sharpen(input_image.data)\n",
        "    \n",
        "    # Normalize the resulting image\n",
        "    optimized_image = input_image.squeeze().detach()\n",
        "    optimized_image = (optimized_image - optimized_image.min()) / (optimized_image.max() - optimized_image.min())\n",
        "    \n",
        "    return optimized_image\n",
        "\n",
        "def sharpen(image):\n",
        "    \"\"\"Apply a sharpening filter to the image.\"\"\"\n",
        "    blurred = gaussian_blur(image, kernel_size=3, sigma=1.0)\n",
        "    sharpened = image + (image - blurred) * 0.5\n",
        "    return torch.clamp(sharpened, 0, 1)\n",
        "\n",
        "def visualize_neuron_activations(model, layer_index, num_neurons=64):\n",
        "    rows = int((num_neurons)**0.5)\n",
        "    cols = (num_neurons + rows - 1) // rows\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    for i in range(num_neurons):\n",
        "        optimized_image = activation_maximization(model, layer_index, i)\n",
        "        axes[i].imshow(optimized_image, cmap='gray')\n",
        "        axes[i].set_title(f'Neuron {i}')\n",
        "        axes[i].axis('off')\n",
        "    \n",
        "    # Hide any unused subplots\n",
        "    for j in range(num_neurons, len(axes)):\n",
        "        axes[j].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'docs/images/MNIST-activation_maximization-layer{layer_index}.png')\n",
        "    plt.show()\n",
        "\n",
        "# Assuming you have already trained your model\n",
        "model = AnalysisMNISTModel()\n",
        "# Load your trained weights here if necessary\n",
        "model.load_state_dict(torch.load('models/mnist_model.pth'))\n",
        "\n",
        "# Example usage\n",
        "visualize_neuron_activations(model, layer_index=0, num_neurons=64)  # Visualize fc2 layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def visualize_feature_maps(model, data_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    for images, _ in data_loader:\n",
        "        with torch.no_grad():\n",
        "            _, fc1_out, fc2_out = model(images)\n",
        "            \n",
        "            # Visualize activations of the first image in the batch\n",
        "            plt.figure(figsize=(12, 5))\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.bar(range(fc1_out.shape[1]), fc1_out[0].numpy())\n",
        "            plt.title('fc1 Feature Map')\n",
        "            \n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.bar(range(fc2_out.shape[1]), fc2_out[0].numpy())\n",
        "            plt.title('fc2 Feature Map')\n",
        "            plt.show()\n",
        "        \n",
        "        break  # Just show for the first batch\n",
        "\n",
        "# Example usage:\n",
        "visualize_feature_maps(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL6Kne88HFOj"
      },
      "source": [
        "## SAE\n",
        "\n",
        "*Activations Shapes:* \n",
        "- fc1: torch.Size([10000, 128])\n",
        "- fc2: torch.Size([10000, 64])\n",
        "- fc3: torch.Size([10000, 10]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LayerConfig:\n",
        "    def __init__(self, name, input_dim):\n",
        "        self.name = name\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "# Create instances for each layer\n",
        "fc1_config = LayerConfig('fc1', 128)\n",
        "fc2_config = LayerConfig('fc2', 64)\n",
        "fc3_config = LayerConfig('fc3', 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simple SAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrVJaAeOhmi6",
        "outputId": "94f61f99-4fbe-480c-b659-2264b4f70426"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class SimpleSAE(nn.Module):\n",
        "    def __init__(self, input_dim=64, hidden_dim=32, l1_coeff=0.1, seed=42):\n",
        "        super(SimpleSAE, self).__init__()\n",
        "        torch.manual_seed(seed)\n",
        "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
        "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
        "        self.l1_coeff = l1_coeff  # L1 regularization coefficient for sparsity\n",
        "        \n",
        "        # Initialize a cache to store lists of activations\n",
        "        self.activation_cache = {\n",
        "            'encoder': [],\n",
        "            'decoder': []\n",
        "        }\n",
        "\n",
        "    def forward(self, x, cache_activations=False):\n",
        "        # Encoder: Reduce the dimensionality\n",
        "        encoded = torch.relu(self.encoder(x))\n",
        "        if cache_activations:\n",
        "            self.activation_cache['encoder'].append(encoded.detach().clone())  # Append encoder activations\n",
        "\n",
        "        # Decoder: Reconstruct the original input\n",
        "        decoded = self.decoder(encoded)\n",
        "        if cache_activations:\n",
        "            self.activation_cache['decoder'].append(decoded.detach().clone())  # Append decoder activations\n",
        "\n",
        "        return encoded, decoded\n",
        "\n",
        "    def compute_loss(self, x, decoded, encoded):\n",
        "        # Reconstruction Loss (MSE)\n",
        "        recon_loss = nn.MSELoss()(decoded, x)\n",
        "\n",
        "        # L1 Sparsity Loss (L1 regularization on encoded activations)\n",
        "        l1_loss = self.l1_coeff * torch.sum(torch.abs(encoded))\n",
        "\n",
        "        # Combine losses: L = MSE + λ * L1\n",
        "        loss = recon_loss + l1_loss\n",
        "\n",
        "        return loss\n",
        "\n",
        "    # Method to retrieve cached activations for a specified layer\n",
        "    def get_cached_activations(self, layer_name):\n",
        "        return torch.cat(self.activation_cache[layer_name]) if layer_name in self.activation_cache else None\n",
        "\n",
        "    # Method to clear the cache\n",
        "    def clear_cache(self):\n",
        "        self.activation_cache = {\n",
        "            'encoder': [],\n",
        "            'decoder': []\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math \n",
        "import torch.nn.functional as F\n",
        "\n",
        "class EnhancedSAE(nn.Module):\n",
        "    def __init__(self, input_dim=64, hidden_dim=32, l1_coeff=0.1, seed=42):\n",
        "        super(EnhancedSAE, self).__init__()\n",
        "        self.l1_coeff = l1_coeff  # L1 regularization coefficient for sparsity\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "        # Encoder and decoder with Kaiming initialization\n",
        "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
        "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "        nn.init.kaiming_uniform_(self.encoder.weight, a=math.sqrt(5))\n",
        "        nn.init.kaiming_uniform_(self.decoder.weight, a=math.sqrt(5))\n",
        "        self.encoder.bias.data.zero_()\n",
        "        self.decoder.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Center the input\n",
        "        x_centered = x - self.decoder.bias\n",
        "        # Encoder: Reduce the dimensionality\n",
        "        encoded = F.relu(self.encoder(x_centered))\n",
        "        # Decoder: Reconstruct the original input\n",
        "        decoded = self.decoder(encoded)\n",
        "\n",
        "        return encoded, decoded\n",
        "\n",
        "    def compute_loss(self, x, decoded, encoded):\n",
        "        # Reconstruction Loss (MSE)\n",
        "        recon_loss = nn.MSELoss()(decoded, x)\n",
        "\n",
        "        # L1 Sparsity Loss (L1 regularization on encoded activations)\n",
        "        l1_loss = self.l1_coeff * torch.sum(torch.abs(encoded))\n",
        "\n",
        "        # Combine losses: L = MSE + λ * L1\n",
        "        loss = recon_loss + l1_loss\n",
        "\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "torch.cat(): expected a non-empty list of Tensors",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m optimizer_sae \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(sae\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Activations loaded from cache\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m train_activations \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cached_activations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselected_layer_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Split the activations into train and test sets (80% train, 20% test)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m train_activations, test_activations \u001b[38;5;241m=\u001b[39m train_test_split(train_activations, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
            "Cell \u001b[0;32mIn[2], line 40\u001b[0m, in \u001b[0;36mMNISTModel.get_cached_activations\u001b[0;34m(self, layer_name)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_cached_activations\u001b[39m(\u001b[38;5;28mself\u001b[39m, layer_name):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m layer_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: torch.cat(): expected a non-empty list of Tensors"
          ]
        }
      ],
      "source": [
        "# Choose the layer configuration you want to use\n",
        "selected_layer_config = fc1_config  # Change this to fc2_config or fc3_config as needed\n",
        "\n",
        "# Use the selected layer configuration\n",
        "input_dim = selected_layer_config.input_dim\n",
        "hidden_dim = 2304\n",
        "sae = SimpleSAE(input_dim=input_dim, hidden_dim=hidden_dim, l1_coeff=0.01)\n",
        "\n",
        "# Optimizer\n",
        "optimizer_sae = optim.Adam(sae.parameters(), lr=learning_rate)\n",
        "\n",
        "# Activations loaded from cache\n",
        "train_activations = model.get_cached_activations(selected_layer_config.name)\n",
        "\n",
        "# Split the activations into train and test sets (80% train, 20% test)\n",
        "train_activations, test_activations = train_test_split(train_activations, test_size=0.2, random_state=42)\n",
        "\n",
        "# DataLoader for activations\n",
        "train_activations_loader = DataLoader(train_activations, batch_size=batch_size, shuffle=True)\n",
        "test_activations_loader = DataLoader(test_activations, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "SAE_test_activations = test_activations #* Test dataset for SAE class\n",
        "\n",
        "# Training Loop for SAE\n",
        "epochs = 5  # You can adjust this based on your preference\n",
        "for epoch in range(epochs):\n",
        "    sae.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_activations_loader:\n",
        "        optimizer_sae.zero_grad()  # Zero out previous gradients\n",
        "\n",
        "        # Forward pass through the SAE\n",
        "        encoded, decoded = sae(batch)\n",
        "\n",
        "        # Compute loss (MSE + sparsity penalty)\n",
        "        loss = sae.compute_loss(batch, decoded, encoded)\n",
        "\n",
        "        loss.backward()  # Backprop for SAE\n",
        "        optimizer_sae.step()  # Optimizer step\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"SAE Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_activations_loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation Loop for SAE\n",
        "sae.eval()\n",
        "total_test_loss = 0\n",
        "with torch.no_grad():\n",
        "    for batch in test_activations_loader:\n",
        "        # Forward pass through the SAE\n",
        "        encoded, decoded = sae(batch)\n",
        "\n",
        "        # Compute loss (MSE + sparsity penalty)\n",
        "        loss = sae.compute_loss(batch, decoded, encoded)\n",
        "\n",
        "        total_test_loss += loss.item()\n",
        "\n",
        "print(f\"SAE Test Loss: {total_test_loss/len(test_activations_loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Activation Maximization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def plot_reconstructions(model, test_data, num_samples=10):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        samples = test_data[:num_samples]\n",
        "        _, reconstructions = model(samples)\n",
        "    \n",
        "    fig, axes = plt.subplots(2, num_samples, figsize=(20, 4))\n",
        "    for i in range(num_samples):\n",
        "        axes[0, i].imshow(samples[i].reshape(8,16), cmap='gray')\n",
        "        axes[0, i].axis('off')\n",
        "        axes[0, i].set_title('Original')\n",
        "        \n",
        "        axes[1, i].imshow(reconstructions[i].reshape(8,16), cmap='gray')\n",
        "        axes[1, i].axis('off')\n",
        "        axes[1, i].set_title('Reconstructed')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_latent_space(model, test_data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        encoded, _ = model(test_data)\n",
        "    \n",
        "    encoded = encoded.numpy()\n",
        "    pca = PCA(n_components=2)\n",
        "    encoded_2d = pca.fit_transform(encoded)\n",
        "    \n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.scatter(encoded_2d[:, 0], encoded_2d[:, 1], alpha=0.5)\n",
        "    plt.title('2D PCA of Latent Space')\n",
        "    plt.xlabel('First Principal Component')\n",
        "    plt.ylabel('Second Principal Component')\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "\n",
        "def plot_feature_activations(model, test_data, num_features=10):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        encoded, _ = model(test_data)\n",
        "    \n",
        "    feature_activations = encoded.mean(dim=0)\n",
        "    top_features = torch.argsort(feature_activations, descending=True)[:num_features]\n",
        "    \n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(range(num_features), feature_activations[top_features])\n",
        "    plt.title(f'Top {num_features} Most Active Features')\n",
        "    plt.xlabel('Feature Index')\n",
        "    plt.ylabel('Average Activation')\n",
        "    plt.show()\n",
        "\n",
        "def compute_cosine_similarity(model, test_data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        encoded, _ = model(test_data)\n",
        "    \n",
        "    encoded = encoded.numpy()\n",
        "    similarity_matrix = cosine_similarity(encoded)\n",
        "    \n",
        "    nonzero_indices = np.nonzero(similarity_matrix)\n",
        "    nonzero_values = similarity_matrix[nonzero_indices]\n",
        "    \n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.scatter(nonzero_indices[0], nonzero_indices[1], c=nonzero_values, cmap='viridis')\n",
        "    plt.colorbar()\n",
        "    plt.title('Nonzero Cosine Similarity of Encoded Representations')\n",
        "    plt.xlabel('Sample Index')\n",
        "    plt.ylabel('Sample Index')\n",
        "    plt.show()\n",
        "\n",
        "def activation_maximization_sae(model, neuron_index, num_iterations=100, learning_rate=0.1):\n",
        "    model.eval()\n",
        "    \n",
        "    # Create a random input\n",
        "    input_data = torch.randn(1, model.encoder.in_features, requires_grad=True)\n",
        "    \n",
        "    optimizer = torch.optim.Adam([input_data], lr=learning_rate)\n",
        "    \n",
        "    for _ in range(num_iterations):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        encoded, _ = model(input_data)\n",
        "        \n",
        "        # Get the activation of the specified neuron\n",
        "        target_activation = encoded[0, neuron_index]\n",
        "        \n",
        "        # Compute the loss (negative activation to maximize it)\n",
        "        loss = -target_activation\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update the input\n",
        "        optimizer.step()\n",
        "    \n",
        "    # Normalize the resulting input\n",
        "    optimized_input = input_data.squeeze().detach()\n",
        "    optimized_input = (optimized_input - optimized_input.min()) / (optimized_input.max() - optimized_input.min())\n",
        "    \n",
        "    return optimized_input\n",
        "\n",
        "def visualize_neuron_activations_sae(model, num_neurons=10):\n",
        "    rows = (num_neurons + 4) // 5  # Calculate the number of rows needed\n",
        "    fig, axes = plt.subplots(rows, 5, figsize=(15, 3 * rows))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    for i in range(num_neurons):\n",
        "        optimized_input = activation_maximization_sae(model, i)\n",
        "        axes[i].imshow(optimized_input.reshape(8, 16), cmap='gray')\n",
        "        axes[i].set_title(f'Neuron {i}')\n",
        "        axes[i].axis('off')\n",
        "    \n",
        "    # Hide any unused subplots\n",
        "    for j in range(num_neurons, len(axes)):\n",
        "        axes[j].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "plot_reconstructions(sae, SAE_test_activations)\n",
        "plot_latent_space(sae, SAE_test_activations)\n",
        "plot_feature_activations(sae, SAE_test_activations)\n",
        "compute_cosine_similarity(sae, SAE_test_activations)\n",
        "visualize_neuron_activations_sae(sae, num_neurons=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What have I done so far? \n",
        "\n",
        "- Created MNIST: ~97% accuracy \n",
        "- Trained two SAEs on MNIST\n",
        "    - SimpleSAE (just encoder decoder)\n",
        "    - ComplexSae (Based off of Neel Nanda's sae when replicating monosemanticity paper) \n",
        "- Simple seems to performs slightly better \n",
        "\n",
        "*Set to a random seed* \n",
        " \n",
        "Next Steps: \n",
        "- Cache all activations when testing the sae\n",
        "- Take the middle layer of the encoder of sae and pass it back into simple sae \n",
        "\n",
        "Confusions: \n",
        "- Should the hidden layer of an SAE be larger than the input\n",
        "    - *What is a hidden layer?* \n",
        "\n",
        "### NEED TO TRACK MAX ACTIVATIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Meta-SAE Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder_config = LayerConfig('encoder', 2304)\n",
        "decoder_config = LayerConfig('decoder', 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cache activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation Loop for SAE\n",
        "sae.clear_cache()\n",
        "sae.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in test_activations_loader:\n",
        "        # Forward pass through the SAE\n",
        "        encoded, decoded = sae(batch, cache_activations=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get cached activations\n",
        "sae.get_cached_activations('encoder').shape, sae.get_cached_activations('decoder').shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "selected_layer_config = encoder_config\n",
        "input_dim = selected_layer_config.input_dim\n",
        "hidden_dim = 49152\n",
        "meta_sae = SimpleSAE(input_dim=input_dim, hidden_dim=hidden_dim, l1_coeff=0.01)\n",
        "\n",
        "optimizer_sae = optim.Adam(meta_sae.parameters(), lr=learning_rate)\n",
        "\n",
        "train_activations = sae.get_cached_activations(selected_layer_config.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "train_activations, test_activations = train_test_split(train_activations, test_size=0.2, random_state=42)\n",
        "\n",
        "META_SAE_test_activations = test_activations #* Test dataset for META_SAE class\n",
        "\n",
        "train_activations_loader = DataLoader(train_activations, batch_size=batch_size, shuffle=True)\n",
        "test_activations_loader = DataLoader(test_activations, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    meta_sae.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_activations_loader:\n",
        "        optimizer_sae.zero_grad()\n",
        "        encoded, decoded = meta_sae(batch)\n",
        "\n",
        "        loss = meta_sae.compute_loss(batch, decoded, encoded)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer_sae.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"SAE Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_activations_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Activation maximization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "\n",
        "def plot_reconstructions(model, test_data, num_samples=10):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        samples = test_data[:num_samples]\n",
        "        _, reconstructions = model(samples)\n",
        "    \n",
        "    fig, axes = plt.subplots(2, num_samples, figsize=(20, 4))\n",
        "    for i in range(num_samples):\n",
        "        axes[0, i].imshow(samples[i].reshape(48,48), cmap='gray')\n",
        "        axes[0, i].axis('off')\n",
        "        axes[0, i].set_title('Original')\n",
        "        \n",
        "        axes[1, i].imshow(reconstructions[i].reshape(48,48), cmap='gray')\n",
        "        axes[1, i].axis('off')\n",
        "        axes[1, i].set_title('Reconstructed')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_latent_space(model, test_data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        encoded, _ = model(test_data)\n",
        "    \n",
        "    encoded = encoded.numpy()\n",
        "    pca = PCA(n_components=2)\n",
        "    encoded_2d = pca.fit_transform(encoded)\n",
        "    \n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.scatter(encoded_2d[:, 0], encoded_2d[:, 1], alpha=0.5)\n",
        "    plt.title('2D PCA of Latent Space')\n",
        "    plt.xlabel('First Principal Component')\n",
        "    plt.ylabel('Second Principal Component')\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "\n",
        "def plot_feature_activations(model, test_data, num_features=10):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        encoded, _ = model(test_data)\n",
        "    \n",
        "    feature_activations = encoded.mean(dim=0)\n",
        "    top_features = torch.argsort(feature_activations, descending=True)[:num_features]\n",
        "    \n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(range(num_features), feature_activations[top_features])\n",
        "    plt.title(f'Top {num_features} Most Active Features')\n",
        "    plt.xlabel('Feature Index')\n",
        "    plt.ylabel('Average Activation')\n",
        "    plt.show()\n",
        "\n",
        "def compute_cosine_similarity(model, test_data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        encoded, _ = model(test_data)\n",
        "    \n",
        "    encoded = encoded.numpy()\n",
        "    similarity_matrix = cosine_similarity(encoded)\n",
        "    \n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(similarity_matrix, cmap='viridis')\n",
        "    plt.colorbar()\n",
        "    plt.title('Cosine Similarity of Encoded Representations')\n",
        "    plt.xlabel('Sample Index')\n",
        "    plt.ylabel('Sample Index')\n",
        "    plt.show()\n",
        "\n",
        "def activation_maximization_sae(model, neuron_index, num_iterations=100, learning_rate=0.1):\n",
        "    model.eval()\n",
        "    \n",
        "    # Create a random input\n",
        "    input_data = torch.randn(1, model.encoder.in_features, requires_grad=True)\n",
        "    \n",
        "    optimizer = torch.optim.Adam([input_data], lr=learning_rate)\n",
        "    \n",
        "    for _ in range(num_iterations):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        encoded, _ = model(input_data)\n",
        "        \n",
        "        # Get the activation of the specified neuron\n",
        "        target_activation = encoded[0, neuron_index]\n",
        "        \n",
        "        # Compute the loss (negative activation to maximize it)\n",
        "        loss = -target_activation\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update the input\n",
        "        optimizer.step()\n",
        "    \n",
        "    # Normalize the resulting input\n",
        "    optimized_input = input_data.squeeze().detach()\n",
        "    optimized_input = (optimized_input - optimized_input.min()) / (optimized_input.max() - optimized_input.min())\n",
        "    \n",
        "    return optimized_input\n",
        "\n",
        "def visualize_neuron_activations_sae(model, num_neurons=10):\n",
        "    rows = (num_neurons + 4) // 5  # Calculate the number of rows needed\n",
        "    fig, axes = plt.subplots(rows, 5, figsize=(15, 3 * rows))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    for i in tqdm(range(num_neurons)):\n",
        "        optimized_input = activation_maximization_sae(model, i)\n",
        "        axes[i].imshow(optimized_input.reshape(48,48), cmap='gray')\n",
        "        axes[i].set_title(f'Neuron {i}')\n",
        "        axes[i].axis('off')\n",
        "    \n",
        "    # Hide any unused subplots\n",
        "    for j in range(num_neurons, len(axes)):\n",
        "        axes[j].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "plot_reconstructions(meta_sae, META_SAE_test_activations)\n",
        "plot_latent_space(meta_sae, META_SAE_test_activations)\n",
        "plot_feature_activations(meta_sae, META_SAE_test_activations)\n",
        "compute_cosine_similarity(meta_sae, META_SAE_test_activations)\n",
        "visualize_neuron_activations_sae(meta_sae, num_neurons=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Notes\n",
        "\n",
        "- meta sae has reallly good loss???\n",
        "- Whats topk and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation\n",
        "\n",
        "How does ViT Prisma do thier emjoi thing? --> Want to track the change in the models understanding of the number over time. \n",
        "wth is a logit --> \n",
        "\n",
        "- Max Activation Evalution\n",
        "\n",
        "1.  MNIST: Run the train set and cache activations \n",
        "        *Store the output labels*\n",
        "2.  Take the labels from the MNIST test and run those activations through the SAE and track max activations of the encoder output layer. \n",
        "3.  Take the acrtivations of the encoder output and track max activations of the encoder output layer \n",
        "\n",
        "(2) (3) can have a function in the simple sae class to help with tracking max activations of the SAE \n",
        "\n",
        "\n",
        "- model -> MNIST \n",
        "- sae -> MNIST Sae \n",
        "- meta-sae -> Sae on MNIST Sae\n",
        "\n",
        "\n",
        "Need to rename all the data loaders to be by class\n",
        "\n",
        "### What data do I have + want?\n",
        "- labels from MNIST\n",
        "- activations of the encoder for each sae\n",
        "- activations of fc1, 2, 3 for mnist \n",
        "\n",
        "- for each activation what is the max neuron and its value --> what was the expected number "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MNIST Max Activations\n",
        "\n",
        "1. Save the top neuron that activated and its value. \n",
        "2. Run the test set through and save the labels. \n",
        "3. After the run and having the cache iterate through the cache and labels \n",
        "4. Add to a df the argmax ( neuron) and max value \n",
        "5. add the projected label as well \n",
        "\n",
        "*Critical Oversight: The model will change its prediction over time and I am not tracking that* "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MNIST_test_dataset #*dataset run through MNIST \n",
        "print(SAE_test_activations) #* MNIST activations run through sae\n",
        "print(META_SAE_test_activations) #* Sae activations run through meta-sae    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Initialize a dictionary to store results\n",
        "results_dict = {\"Max_Value\": [], \"Neuron_Index\": [], \"Predicted_Label\": []}\n",
        "\n",
        "# Set batch size and initialize DataLoader\n",
        "batch_size = 646+\n",
        "test_loader = DataLoader(dataset=MNIST_test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Clear cache at the start and set model to evaluation mode\n",
        "model.clear_cache()\n",
        "model.eval()\n",
        "\n",
        "# Iterate over test data\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # Get outputs and cache activations for this batch\n",
        "        outputs = model(images, cache_activations=True)\n",
        "        \n",
        "        # Predicted labels for the batch\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        \n",
        "        # Process activations from fc1 for the batch\n",
        "        fc1_activations = model.get_cached_activations('fc1')\n",
        "        \n",
        "        # Iterate through the batch to find max activation for each image\n",
        "        for i in range(fc1_activations.size(0)):  # Iterate through batch size\n",
        "            activations = fc1_activations[i]\n",
        "            \n",
        "            # Find the max activation value and corresponding neuron index\n",
        "            max_value, neuron_index = torch.max(activations, 0)\n",
        "            \n",
        "            # Append results to dictionary\n",
        "            results_dict[\"Max_Value\"].append(max_value.item())\n",
        "            results_dict[\"Neuron_Index\"].append(neuron_index.item())\n",
        "            results_dict[\"Predicted_Label\"].append(predicted[i].item())\n",
        "        \n",
        "        # Clear cache manually after processing this batch\n",
        "        model.clear_cache()\n",
        "\n",
        "# Convert the dictionary to a DataFrame\n",
        "MNIST_results_df = pd.DataFrame(results_dict)\n",
        "\n",
        "# After the loop, the DataFrame will contain max activations and neuron indices\n",
        "MNIST_results_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "MNIST_results_df.to_csv('docs/MNIST_results.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SAE Max Activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Initialize a dictionary to store results\n",
        "results_dict = {\"Max_Value\": [], \"Neuron_Index\": []}\n",
        "\n",
        "# Set batch size and initialize DataLoader\n",
        "batch_size = 64\n",
        "test_loader = DataLoader(dataset=SAE_test_activations, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Clear cache at the start and set model to evaluation mode\n",
        "sae.clear_cache()\n",
        "sae.eval()\n",
        "\n",
        "# Iterate over test data\n",
        "with torch.no_grad():\n",
        "    for activations in test_loader:\n",
        "        # Get outputs and cache activations for this batch\n",
        "        outputs = sae(activations, cache_activations=True)\n",
        "        \n",
        "        # Process activations from fc1 for the batch\n",
        "        fc1_activations = sae.get_cached_activations('encoder')\n",
        "        \n",
        "        # Iterate through the batch to find max activation for each image\n",
        "        for i in range(fc1_activations.size(0)):  # Iterate through batch size\n",
        "            activations = fc1_activations[i]\n",
        "            \n",
        "            # Find the max activation value and corresponding neuron index\n",
        "            max_value, neuron_index = torch.max(activations, 0)\n",
        "            \n",
        "            # Append results to dictionary\n",
        "            results_dict[\"Max_Value\"].append(max_value.item())\n",
        "            results_dict[\"Neuron_Index\"].append(neuron_index.item())\n",
        "        \n",
        "        # Clear cache manually after processing this batch\n",
        "        sae.clear_cache()\n",
        "\n",
        "# Convert the dictionary to a DataFrame\n",
        "SAE_results_df = pd.DataFrame(results_dict)\n",
        "SAE_results_df['Predicted_Label'] = MNIST_results_df['Predicted_Label']\n",
        "\n",
        "# After the loop, the DataFrame will contain max activations and neuron indices\n",
        "SAE_results_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "SAE_results_df.to_csv('docs/SAE_results.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Track for each neuron what parts of the weights it is activating on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### META-SAE Max Activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Initialize a dictionary to store results\n",
        "results_dict = {\"Max_Value\": [], \"Neuron_Index\": []}\n",
        "\n",
        "# Set batch size and initialize DataLoader\n",
        "batch_size = 64\n",
        "test_loader = DataLoader(dataset=META_SAE_test_activations, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Clear cache at the start and set model to evaluation mode\n",
        "meta_sae.clear_cache()\n",
        "meta_sae.eval()\n",
        "\n",
        "# Iterate over test data\n",
        "with torch.no_grad():\n",
        "    for activations in test_loader:\n",
        "        # Get outputs and cache activations for this batch\n",
        "        outputs = meta_sae(activations, cache_activations=True)\n",
        "        \n",
        "        # Process activations from fc1 for the batch\n",
        "        fc1_activations = meta_sae.get_cached_activations('encoder')\n",
        "        \n",
        "        # Iterate through the batch to find max activation for each image\n",
        "        for i in range(fc1_activations.size(0)):  # Iterate through batch size\n",
        "            activations = fc1_activations[i]\n",
        "            \n",
        "            # Find the max activation value and corresponding neuron index\n",
        "            max_value, neuron_index = torch.max(activations, 0)\n",
        "            \n",
        "            # Append results to dictionary\n",
        "            results_dict[\"Max_Value\"].append(max_value.item())\n",
        "            results_dict[\"Neuron_Index\"].append(neuron_index.item())\n",
        "        \n",
        "        # Clear cache manually after processing this batch\n",
        "        meta_sae.clear_cache()\n",
        "\n",
        "# Convert the dictionary to a DataFrame\n",
        "META_SAE_results_df = pd.DataFrame(results_dict)\n",
        "META_SAE_results_df['Predicted_Label'] = MNIST_results_df['Predicted_Label']\n",
        "\n",
        "# After the loop, the DataFrame will contain max activations and neuron indices\n",
        "META_SAE_results_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "META_SAE_results_df.to_csv('docs/META_SAE_results.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What have I done so far? \n",
        "\n",
        "-- created the max activation validations\n",
        "- fixed some bugs \n",
        "- cached all activations \n",
        "- looked at fc1 & encoders of both after parsing through it \n",
        "\n",
        "Next Steps: \n",
        "- Analyze the max activations + MAKE GRAPHS!!! (DONE) \n",
        "- Look at Vit-Prisma to see how they track the vision transformer over time \n",
        "- Look at [showing-sae-latents-are-not-atomic-using-meta-saes](https://www.alignmentforum.org/posts/TMAmHh4DdMr4nCSr5/showing-sae-latents-are-not-atomic-using-meta-saes) to see how they analyzed meta saes \n",
        "- automated interpretability [Anthropic Auto-Interp Methods](https://arc.net/l/quote/jukxthen)\n",
        "\n",
        "Confusions: \n",
        "- What else can I do besides max activations to track features + latents?\n",
        "- What happens when I do the decoder instead of the encoder? Can I take something out of the decoder or is the activation just the output. \n",
        "- What is an SAE dictionary size\n",
        "\n",
        "Observations: \n",
        "- The meta-sae is super sparse \n",
        "- the sae is also pretty sparse \n",
        "\n",
        "Future Ideas:\n",
        "- Make it deeper (MNIST & saes)\n",
        "- Batch TOpk? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Goal\n",
        "- Look at ViT Prisma Shape stuff cause that one is prediction dependent\n",
        "- Try doing automated interpretability on MNIST \n",
        "- measure contribution of each feature to the output??"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
