{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x11ff9aed0>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MNISTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNISTModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)  # Input layer (28x28 pixels)\n",
        "        self.fc2 = nn.Linear(128, 64)       # Hidden layer\n",
        "        self.fc3 = nn.Linear(64, 10)        # Output layer (10 classes)\n",
        "\n",
        "        # Initialize a cache to store lists of activations\n",
        "        self.activation_cache = {\n",
        "            'fc1': [],\n",
        "            'fc2': [],\n",
        "            'fc3': []\n",
        "        }\n",
        "\n",
        "    def forward(self, x, cache_activations=False):\n",
        "        x = x.view(-1, 28 * 28)  # Flatten the input image\n",
        "        \n",
        "        # Pass through fc1 and cache activations if needed\n",
        "        fc1_out = torch.relu(self.fc1(x))\n",
        "        if cache_activations:\n",
        "            self.activation_cache['fc1'].append(fc1_out.detach().clone())  # Append fc1 activations\n",
        "        \n",
        "        # Pass through fc2 and cache activations if needed\n",
        "        fc2_out = torch.relu(self.fc2(fc1_out))\n",
        "        if cache_activations:\n",
        "            self.activation_cache['fc2'].append(fc2_out.detach().clone())  # Append fc2 activations\n",
        "        \n",
        "        # Pass through fc3 and cache activations if needed\n",
        "        fc3_out = self.fc3(fc2_out)\n",
        "        if cache_activations:\n",
        "            self.activation_cache['fc3'].append(fc3_out.detach().clone())  # Append fc3 activations\n",
        "\n",
        "        return fc3_out\n",
        "\n",
        "    # Method to retrieve cached activations for a specified layer\n",
        "    def get_cached_activations(self, layer_name):\n",
        "        return torch.cat(self.activation_cache[layer_name]) if layer_name in self.activation_cache else None\n",
        "\n",
        "    # Method to clear the cache\n",
        "    def clear_cache(self):\n",
        "        self.activation_cache = {\n",
        "            'fc1': [],\n",
        "            'fc2': [],\n",
        "            'fc3': []\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IkB3CSGham1e",
        "outputId": "eeb79734-1272-4180-e8f2-a56ec1ccb09a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Loss: 0.3152\n",
            "Epoch [2/5], Loss: 0.1279\n",
            "Epoch [3/5], Loss: 0.0898\n",
            "Epoch [4/5], Loss: 0.0699\n",
            "Epoch [5/5], Loss: 0.0544\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "learning_rate = 0.001\n",
        "epochs = 5\n",
        "\n",
        "# MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # Mean and std deviation of MNIST dataset\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "MNIST_test_dataset = test_dataset #* Test dataset for mnist class \n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize model, loss function and optimizer\n",
        "model = MNISTModel()\n",
        "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "all_activations = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()  # Zero out previous gradients\n",
        "        outputs = model(images)  # Get output and activations from fc2\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Gradient descent\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Store activations for the sparse autoencoder\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 97.58%\n"
          ]
        }
      ],
      "source": [
        "#* Test Accuracy Loop \n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Test accuracy: {100 * correct / total}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cache Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#* Test Accuracy Loop \n",
        "model.clear_cache()\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images, cache_activations=True)\n",
        "        _, predicted = torch.max(outputs.data, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([10000, 128]), torch.Size([10000, 64]), torch.Size([10000, 10]))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.get_cached_activations('fc1').shape, model.get_cached_activations('fc2').shape, model.get_cached_activations('fc3').shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([10000])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_loader.dataset.targets.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL6Kne88HFOj"
      },
      "source": [
        "## SAE\n",
        "\n",
        "*Activations Shapes:* \n",
        "- fc1: torch.Size([10000, 128])\n",
        "- fc2: torch.Size([10000, 64])\n",
        "- fc3: torch.Size([10000, 10]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LayerConfig:\n",
        "    def __init__(self, name, input_dim):\n",
        "        self.name = name\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "# Create instances for each layer\n",
        "fc1_config = LayerConfig('fc1', 128)\n",
        "fc2_config = LayerConfig('fc2', 64)\n",
        "fc3_config = LayerConfig('fc3', 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simple SAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrVJaAeOhmi6",
        "outputId": "94f61f99-4fbe-480c-b659-2264b4f70426"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class SimpleSAE(nn.Module):\n",
        "    def __init__(self, input_dim=64, hidden_dim=32, l1_coeff=0.1, seed=42):\n",
        "        super(SimpleSAE, self).__init__()\n",
        "        torch.manual_seed(seed)\n",
        "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
        "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
        "        self.l1_coeff = l1_coeff  # L1 regularization coefficient for sparsity\n",
        "        \n",
        "        # Initialize a cache to store lists of activations\n",
        "        self.activation_cache = {\n",
        "            'encoder': [],\n",
        "            'decoder': []\n",
        "        }\n",
        "\n",
        "    def forward(self, x, cache_activations=False):\n",
        "        # Encoder: Reduce the dimensionality\n",
        "        encoded = torch.relu(self.encoder(x))\n",
        "        if cache_activations:\n",
        "            self.activation_cache['encoder'].append(encoded.detach().clone())  # Append encoder activations\n",
        "\n",
        "        # Decoder: Reconstruct the original input\n",
        "        decoded = self.decoder(encoded)\n",
        "        if cache_activations:\n",
        "            self.activation_cache['decoder'].append(decoded.detach().clone())  # Append decoder activations\n",
        "\n",
        "        return encoded, decoded\n",
        "\n",
        "    def compute_loss(self, x, decoded, encoded):\n",
        "        # Reconstruction Loss (MSE)\n",
        "        recon_loss = nn.MSELoss()(decoded, x)\n",
        "\n",
        "        # L1 Sparsity Loss (L1 regularization on encoded activations)\n",
        "        l1_loss = self.l1_coeff * torch.sum(torch.abs(encoded))\n",
        "\n",
        "        # Combine losses: L = MSE + λ * L1\n",
        "        loss = recon_loss + l1_loss\n",
        "\n",
        "        return loss\n",
        "\n",
        "    # Method to retrieve cached activations for a specified layer\n",
        "    def get_cached_activations(self, layer_name):\n",
        "        return torch.cat(self.activation_cache[layer_name]) if layer_name in self.activation_cache else None\n",
        "\n",
        "    # Method to clear the cache\n",
        "    def clear_cache(self):\n",
        "        self.activation_cache = {\n",
        "            'encoder': [],\n",
        "            'decoder': []\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math \n",
        "import torch.nn.functional as F\n",
        "\n",
        "class EnhancedSAE(nn.Module):\n",
        "    def __init__(self, input_dim=64, hidden_dim=32, l1_coeff=0.1, seed=42):\n",
        "        super(EnhancedSAE, self).__init__()\n",
        "        self.l1_coeff = l1_coeff  # L1 regularization coefficient for sparsity\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "        # Encoder and decoder with Kaiming initialization\n",
        "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
        "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "        nn.init.kaiming_uniform_(self.encoder.weight, a=math.sqrt(5))\n",
        "        nn.init.kaiming_uniform_(self.decoder.weight, a=math.sqrt(5))\n",
        "        self.encoder.bias.data.zero_()\n",
        "        self.decoder.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Center the input\n",
        "        x_centered = x - self.decoder.bias\n",
        "        # Encoder: Reduce the dimensionality\n",
        "        encoded = F.relu(self.encoder(x_centered))\n",
        "        # Decoder: Reconstruct the original input\n",
        "        decoded = self.decoder(encoded)\n",
        "\n",
        "        return encoded, decoded\n",
        "\n",
        "    def compute_loss(self, x, decoded, encoded):\n",
        "        # Reconstruction Loss (MSE)\n",
        "        recon_loss = nn.MSELoss()(decoded, x)\n",
        "\n",
        "        # L1 Sparsity Loss (L1 regularization on encoded activations)\n",
        "        l1_loss = self.l1_coeff * torch.sum(torch.abs(encoded))\n",
        "\n",
        "        # Combine losses: L = MSE + λ * L1\n",
        "        loss = recon_loss + l1_loss\n",
        "\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SAE Epoch [1/5], Loss: 440.4236\n",
            "SAE Epoch [2/5], Loss: 9.0791\n",
            "SAE Epoch [3/5], Loss: 8.5738\n",
            "SAE Epoch [4/5], Loss: 8.2533\n",
            "SAE Epoch [5/5], Loss: 8.0153\n"
          ]
        }
      ],
      "source": [
        "# Choose the layer configuration you want to use\n",
        "selected_layer_config = fc1_config  # Change this to fc2_config or fc3_config as needed\n",
        "\n",
        "# Use the selected layer configuration\n",
        "input_dim = selected_layer_config.input_dim\n",
        "hidden_dim = 512\n",
        "sae = SimpleSAE(input_dim=input_dim, hidden_dim=hidden_dim, l1_coeff=0.1)\n",
        "\n",
        "# Optimizer\n",
        "optimizer_sae = optim.Adam(sae.parameters(), lr=learning_rate)\n",
        "\n",
        "# Activations loaded from cache\n",
        "train_activations = model.get_cached_activations(selected_layer_config.name)\n",
        "\n",
        "# Split the activations into train and test sets (80% train, 20% test)\n",
        "train_activations, test_activations = train_test_split(train_activations, test_size=0.2, random_state=42)\n",
        "\n",
        "# DataLoader for activations\n",
        "train_activations_loader = DataLoader(train_activations, batch_size=batch_size, shuffle=True)\n",
        "test_activations_loader = DataLoader(test_activations, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "SAE_test_activations = test_activations #* Test dataset for SAE class\n",
        "\n",
        "# Training Loop for SAE\n",
        "epochs = 5  # You can adjust this based on your preference\n",
        "for epoch in range(epochs):\n",
        "    sae.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_activations_loader:\n",
        "        optimizer_sae.zero_grad()  # Zero out previous gradients\n",
        "\n",
        "        # Forward pass through the SAE\n",
        "        encoded, decoded = sae(batch)\n",
        "\n",
        "        # Compute loss (MSE + sparsity penalty)\n",
        "        loss = sae.compute_loss(batch, decoded, encoded)\n",
        "\n",
        "        loss.backward()  # Backprop for SAE\n",
        "        optimizer_sae.step()  # Optimizer step\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"SAE Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_activations_loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SAE Test Loss: 8.0444\n"
          ]
        }
      ],
      "source": [
        "# Evaluation Loop for SAE\n",
        "sae.eval()\n",
        "total_test_loss = 0\n",
        "with torch.no_grad():\n",
        "    for batch in test_activations_loader:\n",
        "        # Forward pass through the SAE\n",
        "        encoded, decoded = sae(batch)\n",
        "\n",
        "        # Compute loss (MSE + sparsity penalty)\n",
        "        loss = sae.compute_loss(batch, decoded, encoded)\n",
        "\n",
        "        total_test_loss += loss.item()\n",
        "\n",
        "print(f\"SAE Test Loss: {total_test_loss/len(test_activations_loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What have I done so far? \n",
        "\n",
        "- Created MNIST: ~97% accuracy \n",
        "- Trained two SAEs on MNIST\n",
        "    - SimpleSAE (just encoder decoder)\n",
        "    - ComplexSae (Based off of Neel Nanda's sae when replicating monosemanticity paper) \n",
        "- Simple seems to performs slightly better \n",
        "\n",
        "*Set to a random seed* \n",
        " \n",
        "Next Steps: \n",
        "- Cache all activations when testing the sae\n",
        "- Take the middle layer of the encoder of sae and pass it back into simple sae \n",
        "\n",
        "Confusions: \n",
        "- Should the hidden layer of an SAE be larger than the input\n",
        "    - *What is a hidden layer?* \n",
        "\n",
        "### NEED TO TRACK MAX ACTIVATIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Meta-SAE Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder_config = LayerConfig('encoder', 32)\n",
        "decoder_config = LayerConfig('decoder', 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cache activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation Loop for SAE\n",
        "sae.clear_cache()\n",
        "sae.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in test_activations_loader:\n",
        "        # Forward pass through the SAE\n",
        "        encoded, decoded = sae(batch, cache_activations=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([2000, 512]), torch.Size([2000, 128]))"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get cached activations\n",
        "sae.get_cached_activations('encoder').shape, sae.get_cached_activations('decoder').shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "selected_layer_config = encoder_config\n",
        "input_dim = selected_layer_config.input_dim\n",
        "hidden_dim = 512\n",
        "meta_sae = SimpleSAE(input_dim=input_dim, hidden_dim=hidden_dim, l1_coeff=0.1)\n",
        "\n",
        "optimizer_sae = optim.Adam(meta_sae.parameters(), lr=learning_rate)\n",
        "\n",
        "train_activations = sae.get_cached_activations(selected_layer_config.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (128x512 and 32x512)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_activations_loader:\n\u001b[1;32m     13\u001b[0m     optimizer_sae\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 14\u001b[0m     encoded, decoded \u001b[38;5;241m=\u001b[39m \u001b[43mmeta_sae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     loss \u001b[38;5;241m=\u001b[39m meta_sae\u001b[38;5;241m.\u001b[39mcompute_loss(batch, decoded, encoded)\n\u001b[1;32m     18\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[0;32m~/Development/Interp/mnist-sae/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Development/Interp/mnist-sae/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[9], line 23\u001b[0m, in \u001b[0;36mSimpleSAE.forward\u001b[0;34m(self, x, cache_activations)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, cache_activations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Encoder: Reduce the dimensionality\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     encoded \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cache_activations:\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_cache[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(encoded\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone())  \u001b[38;5;66;03m# Append encoder activations\u001b[39;00m\n",
            "File \u001b[0;32m~/Development/Interp/mnist-sae/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Development/Interp/mnist-sae/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Development/Interp/mnist-sae/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (128x512 and 32x512)"
          ]
        }
      ],
      "source": [
        "\n",
        "train_activations, test_activations = train_test_split(train_activations, test_size=0.2, random_state=42)\n",
        "\n",
        "META_SAE_test_activations = test_activations #* Test dataset for META_SAE class\n",
        "\n",
        "train_activations_loader = DataLoader(train_activations, batch_size=batch_size, shuffle=True)\n",
        "test_activations_loader = DataLoader(test_activations, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    meta_sae.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_activations_loader:\n",
        "        optimizer_sae.zero_grad()\n",
        "        encoded, decoded = meta_sae(batch)\n",
        "\n",
        "        loss = meta_sae.compute_loss(batch, decoded, encoded)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer_sae.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"SAE Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_activations_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Notes\n",
        "\n",
        "- meta sae has reallly good loss???\n",
        "- Whats topk and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation\n",
        "\n",
        "How does ViT Prisma do thier emjoi thing? --> Want to track the change in the models understanding of the number over time. \n",
        "wth is a logit --> \n",
        "\n",
        "- Max Activation Evalution\n",
        "\n",
        "1.  MNIST: Run the train set and cache activations \n",
        "        *Store the output labels*\n",
        "2.  Take the labels from the MNIST test and run those activations through the SAE and track max activations of the encoder output layer. \n",
        "3.  Take the acrtivations of the encoder output and track max activations of the encoder output layer \n",
        "\n",
        "(2) (3) can have a function in the simple sae class to help with tracking max activations of the SAE \n",
        "\n",
        "\n",
        "- model -> MNIST \n",
        "- sae -> MNIST Sae \n",
        "- meta-sae -> Sae on MNIST Sae\n",
        "\n",
        "\n",
        "Need to rename all the data loaders to be by class\n",
        "\n",
        "### What data do I have + want?\n",
        "- labels from MNIST\n",
        "- activations of the encoder for each sae\n",
        "- activations of fc1, 2, 3 for mnist \n",
        "\n",
        "- for each activation what is the max neuron and its value --> what was the expected number "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MNIST Max Activations\n",
        "\n",
        "1. Save the top neuron that activated and its value. \n",
        "2. Run the test set through and save the labels. \n",
        "3. After the run and having the cache iterate through the cache and labels \n",
        "4. Add to a df the argmax ( neuron) and max value \n",
        "5. add the projected label as well \n",
        "\n",
        "*Critical Oversight: The model will change its prediction over time and I am not tracking that* "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 8.1011,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  3.7614],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  2.7592],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ..., 10.0041,  0.0000,  0.0000],\n",
            "        ...,\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 6.9062,  0.0000,  6.8330,  ...,  1.4161,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  2.6407]])\n",
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "MNIST_test_dataset #*dataset run through MNIST \n",
        "print(SAE_test_activations) #* MNIST activations run through sae\n",
        "print(META_SAE_test_activations) #* Sae activations run through meta-sae    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Max_Value</th>\n",
              "      <th>Neuron_Index</th>\n",
              "      <th>Predicted_Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8.392313</td>\n",
              "      <td>46</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13.096921</td>\n",
              "      <td>63</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.499483</td>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12.808185</td>\n",
              "      <td>92</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10.635261</td>\n",
              "      <td>74</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Max_Value  Neuron_Index  Predicted_Label\n",
              "0   8.392313            46                7\n",
              "1  13.096921            63                2\n",
              "2   7.499483            29                1\n",
              "3  12.808185            92                0\n",
              "4  10.635261            74                4"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Initialize a dictionary to store results\n",
        "results_dict = {\"Max_Value\": [], \"Neuron_Index\": [], \"Predicted_Label\": []}\n",
        "\n",
        "# Set batch size and initialize DataLoader\n",
        "batch_size = 64\n",
        "test_loader = DataLoader(dataset=MNIST_test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Clear cache at the start and set model to evaluation mode\n",
        "model.clear_cache()\n",
        "model.eval()\n",
        "\n",
        "# Iterate over test data\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # Get outputs and cache activations for this batch\n",
        "        outputs = model(images, cache_activations=True)\n",
        "        \n",
        "        # Predicted labels for the batch\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        \n",
        "        # Process activations from fc1 for the batch\n",
        "        fc1_activations = model.get_cached_activations('fc1')\n",
        "        \n",
        "        # Iterate through the batch to find max activation for each image\n",
        "        for i in range(fc1_activations.size(0)):  # Iterate through batch size\n",
        "            activations = fc1_activations[i]\n",
        "            \n",
        "            # Find the max activation value and corresponding neuron index\n",
        "            max_value, neuron_index = torch.max(activations, 0)\n",
        "            \n",
        "            # Append results to dictionary\n",
        "            results_dict[\"Max_Value\"].append(max_value.item())\n",
        "            results_dict[\"Neuron_Index\"].append(neuron_index.item())\n",
        "            results_dict[\"Predicted_Label\"].append(predicted[i].item())\n",
        "        \n",
        "        # Clear cache manually after processing this batch\n",
        "        model.clear_cache()\n",
        "\n",
        "# Convert the dictionary to a DataFrame\n",
        "MNIST_results_df = pd.DataFrame(results_dict)\n",
        "\n",
        "# After the loop, the DataFrame will contain max activations and neuron indices\n",
        "MNIST_results_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "MNIST_results_df.to_csv('docs/MNIST_results.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SAE Max Activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Max_Value</th>\n",
              "      <th>Neuron_Index</th>\n",
              "      <th>Predicted_Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Max_Value  Neuron_Index  Predicted_Label\n",
              "0        0.0             0                7\n",
              "1        0.0             0                2\n",
              "2        0.0             0                1\n",
              "3        0.0             0                0\n",
              "4        0.0             0                4"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Initialize a dictionary to store results\n",
        "results_dict = {\"Max_Value\": [], \"Neuron_Index\": []}\n",
        "\n",
        "# Set batch size and initialize DataLoader\n",
        "batch_size = 64\n",
        "test_loader = DataLoader(dataset=SAE_test_activations, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Clear cache at the start and set model to evaluation mode\n",
        "sae.clear_cache()\n",
        "sae.eval()\n",
        "\n",
        "# Iterate over test data\n",
        "with torch.no_grad():\n",
        "    for activations in test_loader:\n",
        "        # Get outputs and cache activations for this batch\n",
        "        outputs = sae(activations, cache_activations=True)\n",
        "        \n",
        "        # Process activations from fc1 for the batch\n",
        "        fc1_activations = sae.get_cached_activations('encoder')\n",
        "        \n",
        "        # Iterate through the batch to find max activation for each image\n",
        "        for i in range(fc1_activations.size(0)):  # Iterate through batch size\n",
        "            activations = fc1_activations[i]\n",
        "            \n",
        "            # Find the max activation value and corresponding neuron index\n",
        "            max_value, neuron_index = torch.max(activations, 0)\n",
        "            \n",
        "            # Append results to dictionary\n",
        "            results_dict[\"Max_Value\"].append(max_value.item())\n",
        "            results_dict[\"Neuron_Index\"].append(neuron_index.item())\n",
        "        \n",
        "        # Clear cache manually after processing this batch\n",
        "        sae.clear_cache()\n",
        "\n",
        "# Convert the dictionary to a DataFrame\n",
        "SAE_results_df = pd.DataFrame(results_dict)\n",
        "SAE_results_df['Predicted_Label'] = MNIST_results_df['Predicted_Label']\n",
        "\n",
        "# After the loop, the DataFrame will contain max activations and neuron indices\n",
        "SAE_results_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "SAE_results_df.to_csv('docs/SAE_results.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### META-SAE Max Activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Max_Value</th>\n",
              "      <th>Neuron_Index</th>\n",
              "      <th>Predicted_Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.029132</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.029132</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.029132</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.029132</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.029132</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Max_Value  Neuron_Index  Predicted_Label\n",
              "0   0.029132             4                7\n",
              "1   0.029132             4                2\n",
              "2   0.029132             4                1\n",
              "3   0.029132             4                0\n",
              "4   0.029132             4                4"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Initialize a dictionary to store results\n",
        "results_dict = {\"Max_Value\": [], \"Neuron_Index\": []}\n",
        "\n",
        "# Set batch size and initialize DataLoader\n",
        "batch_size = 64\n",
        "test_loader = DataLoader(dataset=META_SAE_test_activations, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Clear cache at the start and set model to evaluation mode\n",
        "meta_sae.clear_cache()\n",
        "meta_sae.eval()\n",
        "\n",
        "# Iterate over test data\n",
        "with torch.no_grad():\n",
        "    for activations in test_loader:\n",
        "        # Get outputs and cache activations for this batch\n",
        "        outputs = meta_sae(activations, cache_activations=True)\n",
        "        \n",
        "        # Process activations from fc1 for the batch\n",
        "        fc1_activations = meta_sae.get_cached_activations('encoder')\n",
        "        \n",
        "        # Iterate through the batch to find max activation for each image\n",
        "        for i in range(fc1_activations.size(0)):  # Iterate through batch size\n",
        "            activations = fc1_activations[i]\n",
        "            \n",
        "            # Find the max activation value and corresponding neuron index\n",
        "            max_value, neuron_index = torch.max(activations, 0)\n",
        "            \n",
        "            # Append results to dictionary\n",
        "            results_dict[\"Max_Value\"].append(max_value.item())\n",
        "            results_dict[\"Neuron_Index\"].append(neuron_index.item())\n",
        "        \n",
        "        # Clear cache manually after processing this batch\n",
        "        meta_sae.clear_cache()\n",
        "\n",
        "# Convert the dictionary to a DataFrame\n",
        "META_SAE_results_df = pd.DataFrame(results_dict)\n",
        "META_SAE_results_df['Predicted_Label'] = MNIST_results_df['Predicted_Label']\n",
        "\n",
        "# After the loop, the DataFrame will contain max activations and neuron indices\n",
        "META_SAE_results_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "META_SAE_results_df.to_csv('docs/META_SAE_results.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What have I done so far? \n",
        "\n",
        "-- created the max activation validations\n",
        "- fixed some bugs \n",
        "- cached all activations \n",
        "- looked at fc1 & encoders of both after parsing through it \n",
        "\n",
        "Next Steps: \n",
        "- Analyze the max activations + MAKE GRAPHS!!!\n",
        "- Look at Vit-Prisma to see how they track the vision transformer over time \n",
        "- Look at [showing-sae-latents-are-not-atomic-using-meta-saes](https://www.alignmentforum.org/posts/TMAmHh4DdMr4nCSr5/showing-sae-latents-are-not-atomic-using-meta-saes) to see how they analyzed meta saes \n",
        "\n",
        "Confusions: \n",
        "- What else can I do besides max activations to track features + latents?\n",
        "- What happens when I do the decoder instead of the encoder? Can I take something out of the decoder or is the activation just the output. \n",
        "- What is an SAE dictionary size\n",
        "\n",
        "Observations: \n",
        "- The meta-sae is super sparse \n",
        "- the sae is also pretty sparse \n",
        "\n",
        "Future Ideas:\n",
        "- Make it deeper (MNIST & saes)\n",
        "- Batch TOpk? "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
