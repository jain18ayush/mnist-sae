{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the SAE and Models for Meta-SAE on decoder WEIGHTS (not activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Env Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from structs.models import CIFAR100Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "path = '/Volumes/Ayush_Drive/mnist/'\n",
    "\n",
    "if os.path.exists(path):\n",
    "    prefix = path\n",
    "else:\n",
    "    prefix = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root=f'{prefix}/data'\n",
    "\n",
    "# Load CIFAR-100 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load training data\n",
    "train_dataset = datasets.CIFAR100(\n",
    "    root=root,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Load test data\n",
    "test_dataset = datasets.CIFAR100(\n",
    "    root=root,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = torch.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=100):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        \n",
    "        # This is specifically modified for CIFAR: \n",
    "        # Smaller initial conv with 3x3 kernel instead of 7x7\n",
    "        # No initial max pooling to preserve spatial information\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        \n",
    "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x, cache_activations=False):\n",
    "        # If caching is enabled, we'll store activations\n",
    "        activations = {}\n",
    "        \n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        if cache_activations:\n",
    "            activations['conv1'] = out.detach().clone()\n",
    "            \n",
    "        out = self.layer1(out)\n",
    "        if cache_activations:\n",
    "            activations['layer1'] = out.detach().clone()\n",
    "            \n",
    "        out = self.layer2(out)\n",
    "        if cache_activations:\n",
    "            activations['layer2'] = out.detach().clone()\n",
    "            \n",
    "        out = self.layer3(out)\n",
    "        if cache_activations:\n",
    "            activations['layer3'] = out.detach().clone()\n",
    "            \n",
    "        out = self.layer4(out)\n",
    "        if cache_activations:\n",
    "            activations['layer4'] = out.detach().clone()\n",
    "            \n",
    "        out = torch.nn.functional.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        fc_features = out\n",
    "        if cache_activations:\n",
    "            activations['fc_features'] = fc_features.detach().clone()\n",
    "            \n",
    "        out = self.linear(out)\n",
    "        if cache_activations:\n",
    "            activations['output'] = out.detach().clone()\n",
    "            \n",
    "        if cache_activations:\n",
    "            return out, activations\n",
    "        return out\n",
    "\n",
    "# Create ResNet18 model\n",
    "def ResNet18(num_classes=100):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n",
    "#* This is the model for CIFAR100 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded successfully from /Volumes/Ayush_Drive/mnist//embeddings/cifar100/cifar100_model.pth\n",
      "Model was trained for 25 epochs with accuracy: 58.85\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from structs.models import ResNet, ResNet18, BasicBlock\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer, scheduler):\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('mps'))\n",
    "    \n",
    "    # Load model weights\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Load optimizer state\n",
    "    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    # # Load scheduler state\n",
    "    # scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    # Get the epoch and accuracy information\n",
    "    epoch = checkpoint['epoch']\n",
    "    accuracy = checkpoint['accuracy']\n",
    "    \n",
    "    # Return important values that might be needed for resuming training\n",
    "    return model, epoch, accuracy\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = ResNet18()\n",
    "# load in model from checkpoint\n",
    "checkpoint_path = f'{prefix}/embeddings/cifar100/cifar100_model.pth'\n",
    "if os.path.exists(checkpoint_path):\n",
    "    model, epoch, accuracy = load_checkpoint(checkpoint_path, model, optimizer=None, scheduler=None)\n",
    "    print(f\"Checkpoint loaded successfully from {checkpoint_path}\")\n",
    "    print(f\"Model was trained for {epoch} epochs with accuracy: {accuracy}\")\n",
    "else:\n",
    "    raise Exception(f\"Checkpoint not found at {checkpoint_path}\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting activations: 100%|██████████| 938/938 [25:54<00:00,  1.66s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "\n",
    "def extract_activations(model, dataset, layer_names=None, batch_size=64, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"\n",
    "    Extract activations from specified layers of a neural network.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model (torch.nn.Module)\n",
    "        dataset: The dataset to extract activations from\n",
    "        layer_names: List of layer names to extract activations from. If None, extracts from all layers with hooks\n",
    "        batch_size: Batch size for data loading\n",
    "        device: Device to run the model on ('cuda' or 'cpu')\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping layer names to their activations (tensors)\n",
    "    \"\"\"\n",
    "    # Move model to device and set to evaluation mode\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Create data loader\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Dictionary to store activations\n",
    "    activations = collections.defaultdict(list)\n",
    "    \n",
    "    # If the model has a built-in activation cache mechanism (like your MNIST model)\n",
    "    if hasattr(model, 'clear_cache') and hasattr(model, 'get_cached_activations'):\n",
    "        model.clear_cache()\n",
    "        \n",
    "        # Run forward pass through the model to populate cache\n",
    "        with torch.no_grad():\n",
    "            for inputs, _ in tqdm(data_loader, desc=\"Extracting activations\"):\n",
    "                inputs = inputs.to(device)\n",
    "                _ = model(inputs)\n",
    "        \n",
    "        # Get activations from cache\n",
    "        if layer_names is None:\n",
    "            # Get all available layer activations\n",
    "            layer_names = list(model.activation_cache.keys())\n",
    "        \n",
    "        for layer_name in layer_names:\n",
    "            if layer_name in model.activation_cache:\n",
    "                layer_activations = model.get_cached_activations(layer_name)\n",
    "                activations[layer_name] = layer_activations\n",
    "    \n",
    "    # For models without built-in caching (like ResNet), use hooks\n",
    "    else:\n",
    "        # Storage for hooks\n",
    "        hooks = []\n",
    "        \n",
    "        # Set up forward hooks\n",
    "        def get_activation(name):\n",
    "            def hook(module, input, output):\n",
    "                # For convolutional layers, flatten spatial dimensions\n",
    "                if len(output.shape) == 4:  # [batch_size, channels, height, width]\n",
    "                    # Keep batch dimension and flatten the rest\n",
    "                    flattened = output.view(output.size(0), -1)\n",
    "                    activations[name].append(flattened.cpu().detach())\n",
    "                else:\n",
    "                    activations[name].append(output.cpu().detach())\n",
    "            return hook\n",
    "        \n",
    "        # Register hooks for specified layers\n",
    "        if layer_names:\n",
    "            for name, module in model.named_modules():\n",
    "                if name in layer_names:\n",
    "                    hooks.append(module.register_forward_hook(get_activation(name)))\n",
    "        else:\n",
    "            # If no specific layers are requested, hook into all possible layers\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, (torch.nn.Conv2d, torch.nn.Linear, torch.nn.BatchNorm2d)) or \"layer\" in name:\n",
    "                    hooks.append(module.register_forward_hook(get_activation(name)))\n",
    "        \n",
    "        # Run forward pass\n",
    "        with torch.no_grad():\n",
    "            for inputs, _ in tqdm(data_loader, desc=\"Extracting activations\"):\n",
    "                inputs = inputs.to(device)\n",
    "                _ = model(inputs)\n",
    "        \n",
    "        # Concatenate batched activations and remove hooks\n",
    "        for name in activations:\n",
    "            activations[name] = torch.cat(activations[name], dim=0)\n",
    "        \n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "    \n",
    "    # Convert from defaultdict to regular dict\n",
    "    return dict(activations)\n",
    "\n",
    "concat_dataset = torch.utils.data.ConcatDataset([train_dataset, test_dataset])\n",
    "# Example usage:\n",
    "all_activations = extract_activations(model, concat_dataset, layer_names=['layer3', 'layer4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 3 activations shape: torch.Size([60000, 16384])\n",
      "Layer 4 activations shape: torch.Size([60000, 8192])\n"
     ]
    }
   ],
   "source": [
    "layer3_activations = all_activations['layer3']\n",
    "layer4_activations = all_activations['layer4']\n",
    "# Check the shape of the activations\n",
    "print(f\"Layer 3 activations shape: {layer3_activations.shape}\")\n",
    "print(f\"Layer 4 activations shape: {layer4_activations.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(layer3_activations, f'{prefix}/embeddings/cifar100/layer3_activations.pth')\n",
    "torch.save(layer4_activations, f'{prefix}/embeddings/cifar100/layer4_activations.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add loading code only if needed\n",
    "# Load the activations from file\n",
    "if layer3_activations is None:\n",
    "    layer3_activations = torch.load(f'{prefix}/embeddings/cifar100/layer3_activations.pth')\n",
    "if layer4_activations is None:\n",
    "    layer4_activations = torch.load(f'{prefix}/embeddings/cifar100/layer4_activations.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
