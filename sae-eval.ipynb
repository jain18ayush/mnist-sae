{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Ayush_Drive/mnist/\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "path = '/Volumes/Ayush_Drive/mnist/'\n",
    "\n",
    "if os.path.exists(path):\n",
    "    prefix = path\n",
    "else:\n",
    "    prefix = ''\n",
    "\n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from structs.models import SimpleSAE\n",
    "from structs.utils import fc1_config, encoder_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measurements \n",
    "\n",
    "- Have the EMNIST ones \n",
    "- Need to load in each set of models and each set of embeddings \n",
    "- or run the embeddings again? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'EMNIST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading activations:   0%|          | 0/9 [00:00<?, ?it/s]/var/folders/24/njx_3v7n0nj9kkzrv028sllm0000gn/T/ipykernel_35908/3859919466.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  activations[depth] = torch.load(filename)\n",
      "Loading activations: 100%|██████████| 9/9 [01:15<00:00,  8.39s/it]\n"
     ]
    }
   ],
   "source": [
    "# load in the activations in a dictionary \n",
    "import torch\n",
    "from tqdm import tqdm \n",
    "\n",
    "activations = {}\n",
    "for depth in tqdm(range(1, 10), desc=\"Loading activations\"): \n",
    "    filename = f'{prefix}embeddings/mnist_encoder_{data_name}_depth_{depth}.pth'\n",
    "    activations[depth] = torch.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([112800, 2304])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L0 Sparsity\n",
    "\n",
    "- the average number of nonzero feature activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get it for one depth \n",
    "def calc_l0(activation_vector):\n",
    "    return (activation_vector != 0).sum() / activation_vector.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc the average l0 sparsity per depth \n",
    "l0_sparsity = {}\n",
    "for depth in activations.keys():\n",
    "    l0_sparsity[depth] = calc_l0(activations[depth])\n",
    "    print(f\"Depth {depth}: {l0_sparsity[depth]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the l0 sparsity\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(list(l0_sparsity.keys()), list(l0_sparsity.values()))\n",
    "plt.xlabel('Depth')\n",
    "plt.ylabel('L0 Sparsity')\n",
    "plt.title('L0 Sparsity vs Depth')\n",
    "plt.savefig(f\"plots/saebench-metrics/{data_name}_l0_sparsity.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l0_sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dictionary as a json \n",
    "\n",
    "import json\n",
    "\n",
    "l0_sparsity_serializable = {str(k): float(v) for k, v in l0_sparsity.items()}\n",
    "\n",
    "data_object = {\n",
    "    \"data_name\" : f\"{data_name}\",\n",
    "    \"metrics\" : {\n",
    "        \"l0_sparsity\" : l0_sparsity_serializable\n",
    "    }\n",
    "}\n",
    "\n",
    "json.dump(data_object, open(f'{prefix}data/{data_name}.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring impact on loss\n",
    "\n",
    "- Get model activations at layer L\n",
    "- Pass through SAE to get reconstruction\n",
    "- Replace original activations with reconstruction\n",
    "- Continue model forward pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For MNIST we are just measuring the sae, then we recursively measure essentially "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Activations  --> colored SAE? \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class ToRGB:\n",
    "    def __call__(self, img):\n",
    "        return img.repeat(3, 1, 1)  # Repeat the grayscale channel 3 times\n",
    "\n",
    "# Updated transforms for colored MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    ToRGB(),  # Convert to RGB\n",
    "    transforms.Normalize(mean=[0.1307, 0.1307, 0.1307],  # Same normalization for each channel\n",
    "                       std=[0.3081, 0.3081, 0.3081])\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.MNIST(root=f'{prefix}/data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root=f'{prefix}/data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from structs.models import MNISTModel, ColoredMNISTModel\n",
    "# load in trained mnist model \n",
    "model = ColoredMNISTModel()\n",
    "model.load_state_dict(torch.load(f'{prefix}models/colored_mnist_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Metrics\n",
    "- Validation accuracy: 0.79%\n",
    "- Average loss: 0.01\n",
    "\n",
    "#### Depth 1 Metrics \n",
    "- Validation accuracy: 0.18%\n",
    "- Average loss: 0.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.clear_cache()\n",
    "model.eval()\n",
    "total_correct = 0\n",
    "total_loss = 0\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = total_correct / len(test_loader.dataset)\n",
    "average_loss = total_loss / len(test_loader.dataset)\n",
    "print(f'Validation accuracy: {accuracy:.2f}%')\n",
    "print(f'Average loss: {average_loss:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOOK Testing \n",
    "# baseline is there , now need to replace the activations in the model internally\n",
    "def basic_hook(module, input, output):\n",
    "    # output here is after the fc1 layer but before ReLU\n",
    "    # Any changes you return will be what gets ReLU'd and passed to fc2\n",
    "    print(f\"input: {input[0].shape}\")\n",
    "    print(f\"output: {output.shape}\")\n",
    "    print(f\"module: {module}\")\n",
    "    return output # Pass through unchanged\n",
    "\n",
    "# Replace the hook\n",
    "basic_handle = model.fc1.register_forward_hook(basic_hook)\n",
    "basic_handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hook_with_sae(sae_model, depth):\n",
    "    def hook(module, input, output):\n",
    "        encoded, reconstructed = sae_model(output)\n",
    "        return reconstructed\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from structs.models import SimpleSAE\n",
    "from structs.utils import fc1_config, encoder_config\n",
    "sae_model = SimpleSAE(input_dim=fc1_config.input_dim, hidden_dim=encoder_config.input_dim) \n",
    "sae_model.load_state_dict(torch.load(f'{prefix}models/mnist-colored_sae_MNIST_depth_1.pth'))\n",
    "sae_model.clear_cache()\n",
    "\n",
    "handle = model.fc1.register_forward_hook(create_hook_with_sae(sae_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.clear_cache()\n",
    "model.eval()\n",
    "\n",
    "total_correct = 0\n",
    "total_loss = 0\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = total_correct / len(test_loader.dataset)\n",
    "average_loss = total_loss / len(test_loader.dataset)\n",
    "print(f'Validation accuracy: {accuracy:.2f}%')\n",
    "print(f'Average loss: {average_loss:.2f}')\n",
    "\n",
    "handle.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in SAE Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/24/njx_3v7n0nj9kkzrv028sllm0000gn/T/ipykernel_35908/4138239383.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sae_model.load_state_dict(torch.load(f'{prefix}models/mnist-colored_sae_MNIST_depth_{depth}.pth'))\n"
     ]
    }
   ],
   "source": [
    "# sae at depth 1 , 2, 3, 4, 5, 6, 7, 8, 9\n",
    "sae_models = {}\n",
    "for depth in range(1, 10):\n",
    "    if depth == 1:\n",
    "        sae_model = SimpleSAE(input_dim=fc1_config.input_dim, hidden_dim=encoder_config.input_dim) \n",
    "    else:\n",
    "        sae_model = SimpleSAE(input_dim=encoder_config.input_dim, hidden_dim=encoder_config.input_dim)\n",
    "    sae_model.load_state_dict(torch.load(f'{prefix}models/mnist-colored_sae_MNIST_depth_{depth}.pth'))\n",
    "    sae_model.clear_cache()\n",
    "    sae_models[depth] = sae_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at the impact of each successive sae model on the loss of the previous \n",
    "\n",
    "- establish baseline loss for each model \n",
    "- then establish the loss difference with the reconstruction just below \n",
    "- then establish the loss difference through successive layers\n",
    "\n",
    "**This is using EMNIST Embeddings instead of MNIST** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to construct some hook that gets the reconstruction from depth 1 but through the entire network\n",
    "# first for each sae lets just get the loss difference from base and when substituted with a forward pass through the sae \n",
    "# then we can do the same for the entire network\n",
    "\n",
    "def sae_pass_hook(sae_model): # for some model at some depth \n",
    "    def hook(module, input, output):\n",
    "        encoded, reconstructed = sae_model(output)\n",
    "        return reconstructed\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "#pass in an sae and the test_loader contains the activations of the sae above it (that it is fed)\n",
    "\n",
    "def measure_loss(model, test_loader, criterion=nn.CrossEntropyLoss()):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for activations, labels in test_loader:\n",
    "            encoded, decoded = model(activations)\n",
    "            loss = criterion(encoded, activations)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(test_loader.dataset)\n",
    "\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024787288249886415"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = activations[1]\n",
    "dataset = torch.utils.data.TensorDataset(dataset, dataset)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "measure_loss(sae_models[2], test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring Loss: 100%|██████████| 1/1 [00:18<00:00, 18.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth 8 Loss: {'before_loss': 0.0, 'after_loss': 0.0, 'loss_diff': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "try:\n",
    "    loss = json.load(open(f'{prefix}metrics/{data_name}_loss.json'))\n",
    "    start_depth = int(max(loss.keys())) + 1\n",
    "except (FileNotFoundError, ValueError):\n",
    "    loss = {}\n",
    "    start_depth = 2\n",
    "\n",
    "\n",
    "for depth in tqdm(range(start_depth, 9), desc=\"Measuring Loss\"):\n",
    "    sae_obj = sae_models[depth]\n",
    "\n",
    "    dataset = activations[depth - 1]\n",
    "    dataset = torch.utils.data.TensorDataset(dataset, dataset)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    before_loss = measure_loss(sae_obj, test_loader); \n",
    "\n",
    "    next_sae_obj = sae_models[depth + 1]\n",
    "    handle = sae_obj.encoder.register_forward_hook(sae_pass_hook(next_sae_obj))\n",
    "\n",
    "    after_loss = measure_loss(sae_obj, test_loader); \n",
    "\n",
    "    loss[str(depth)] = {  # Convert depth to string for JSON\n",
    "        \"before_loss\": before_loss,  # Convert tensor to float\n",
    "        \"after_loss\": after_loss,\n",
    "        \"loss_diff\": (after_loss - before_loss)\n",
    "    }\n",
    "    \n",
    "    # Save after each iteration\n",
    "    handle.remove()\n",
    "\n",
    "    json.dump(loss, open(f'{prefix}metrics/{data_name}_loss.json', 'w'))\n",
    "\n",
    "    print(f\"Depth {depth} Loss: {loss[str(depth)]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
