{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x116987850>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerConfig:\n",
    "    def __init__(self, name, input_dim):\n",
    "        self.name = name\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "# Create instances for each layer\n",
    "fc1_config = LayerConfig('fc1', 256)\n",
    "fc2_config = LayerConfig('fc2', 128)\n",
    "fc3_config = LayerConfig('fc3', 10)\n",
    "encoder_config = LayerConfig('encoder', 2304)\n",
    "decoder_config = LayerConfig('decoder', 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(28, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Normalization for validation\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "data_name = 'EMNIST'\n",
    "\n",
    "# Load CIFAR10 dataset\n",
    "# train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "# val_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_val)\n",
    "\n",
    "# Load CIFAR100 dataset\n",
    "# train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "# val_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_val)\n",
    "\n",
    "# Load EMNIST dataset\n",
    "train_dataset = datasets.EMNIST(root='./data', split='balanced', train=True, download=True, transform=transform_train)\n",
    "val_dataset = datasets.EMNIST(root='./data', split='balanced', train=False, download=True, transform=transform_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    # Undo normalization\n",
    "    mean = np.array([0.4914, 0.4822, 0.4465])\n",
    "    std = np.array([0.2023, 0.1994, 0.2010])\n",
    "    \n",
    "    # Convert to numpy and transpose from (C,H,W) to (H,W,C)\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    # Unnormalize\n",
    "    img = std * img + mean\n",
    "    \n",
    "    # Clip values to be between 0 and 1\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "# Get a random image from the training set\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# # Display a single image\n",
    "# plt.figure(figsize=(4, 4))\n",
    "# imshow(images[0])\n",
    "# plt.title(f'Class: {labels[0].item()}')\n",
    "# plt.show()\n",
    "\n",
    "# If you want to display the actual class name\n",
    "classes = train_dataset.classes  # Get class names\n",
    "plt.figure(figsize=(4, 4))\n",
    "imshow(images[0])\n",
    "plt.title(f'Class: {classes[labels[0].item()]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from structs.models import MNISTModel, ColoredMNISTModel\n",
    "# load in trained mnist model \n",
    "model = ColoredMNISTModel()\n",
    "model.load_state_dict(torch.load('models/mnist_colored.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_layer_config = fc1_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.clear_cache()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        outputs = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = len(train_loader.dataset)\n",
    "print(f'Number of images in the train_loader: {num_images}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_activations = model.get_cached_activations(selected_layer_config.name)\n",
    "analysis_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(analysis_activations, f'embeddings/mnist_{selected_layer_config.name}_{data_name}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sae Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from structs.models import SimpleSAE, EnhancedSAE\n",
    "\n",
    "input_dim = selected_layer_config.input_dim\n",
    "hidden_dim = 2304\n",
    "sae = EnhancedSAE(input_dim=input_dim, hidden_dim=hidden_dim, l1_coeff=0.01)\n",
    "sae.load_state_dict(torch.load('models/mnist_sae_colored.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_activations = torch.load(f'embeddings/mnist_{fc1_config.name}_{data_name}.pth')\n",
    "analysis_loader = DataLoader(analysis_activations, batch_size=batch_size, shuffle=False) # do not shuffle\n",
    "\n",
    "analysis_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the activations through the SAE and save the intermediary activations \n",
    "sae.clear_cache()\n",
    "sae.eval()\n",
    "with torch.no_grad():\n",
    "    for activations in tqdm(analysis_loader):\n",
    "        encoded, decoded = sae(activations, cache_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_activations = sae.get_cached_activations('encoder')\n",
    "torch.save(sae_activations, f'embeddings/mnist_encoder_{data_name}.pth')\n",
    "sae_activations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from structs.models import SimpleSAE, EnhancedSAE\n",
    "selected_layer_config = encoder_config\n",
    "\n",
    "input_dim = selected_layer_config.input_dim\n",
    "hidden_dim = 2304\n",
    "meta_sae = EnhancedSAE(input_dim=input_dim, hidden_dim=hidden_dim, l1_coeff=0.01)\n",
    "meta_sae.load_state_dict(torch.load('models/mnist_meta_sae_colored.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_analysis_activations = torch.load(f'embeddings/mnist_encoder_{data_name}.pth')\n",
    "meta_analysis_loader = DataLoader(meta_analysis_activations, batch_size=batch_size, shuffle=False) # do not shuffle\n",
    "\n",
    "meta_analysis_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the activations through the SAE and save the intermediary activations \n",
    "meta_sae.clear_cache()\n",
    "meta_sae.eval()\n",
    "with torch.no_grad():\n",
    "    for activations in tqdm(meta_analysis_loader):\n",
    "        encoded, decoded = meta_sae(activations, cache_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_sae_activations = meta_sae.get_cached_activations('encoder')\n",
    "meta_sae_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(meta_sae_activations, f'embeddings/mnist_meta_encoder_{data_name}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_activations = torch.load(f'embeddings/mnist_meta_encoder_{data_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose is to make it by neuron as opposed to by sample \n",
    "transposed = sae_activations.T\n",
    "transposed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_10_indices_per_neuron = torch.argsort(transposed, descending=True, dim=1)[:, :10]\n",
    "max_10_indices_per_neuron_value = torch.gather(transposed, 1, max_10_indices_per_neuron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_indices_save(indices, filepath, filename, neuron_idx):\n",
    "    plt.figure(figsize=(15, 4))  # Adjusted figure size for 10 images\n",
    "    count = 0\n",
    "    for i, idx in enumerate(indices):\n",
    "        activation_value = transposed[neuron_idx][idx]\n",
    "        if transposed[neuron_idx][idx] == 0:\n",
    "            # print('skipped idx', idx)\n",
    "            continue\n",
    "        count += 1\n",
    "\n",
    "\n",
    "        img = train_dataset.data[idx]\n",
    "        \n",
    "        plt.subplot(2, 5, i+1)  # 2 rows, 5 columns\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Index: {idx} - Value: {activation_value:.2f}')\n",
    "\n",
    "\n",
    "    if plt.gca().has_data():\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{filepath}/{count}_{filename}')  # Save the figure to a file\n",
    "    plt.close()  # Close the figure to free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os \n",
    "\n",
    "filepath = f'docs/neuron_{data_name}_meta'\n",
    "os.makedirs(filepath, exist_ok=True)\n",
    "\n",
    "for i in tqdm(range(len(max_10_indices_per_neuron)), desc=\"Plotting neurons\"):\n",
    "    indices = max_10_indices_per_neuron[i]\n",
    "    plot_indices_save(indices, filepath, f'neuron_{i}_plots.png', i)  # Save individual plots if needed\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
