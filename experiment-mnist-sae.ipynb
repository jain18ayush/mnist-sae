{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x117f87850>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerConfig:\n",
    "    def __init__(self, name, input_dim):\n",
    "        self.name = name\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "# Create instances for each layer\n",
    "fc1_config = LayerConfig('fc1', 256)\n",
    "fc2_config = LayerConfig('fc2', 128)\n",
    "fc3_config = LayerConfig('fc3', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dataset not found. You can use download=True to download it",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 34\u001b[0m\n\u001b[1;32m     27\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mCIFAR10(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform_val)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Load CIFAR100 dataset\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# val_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_val)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Load EuroSAT dataset\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEuroSAT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# val_dataset = datasets.EuroSAT(root='./data', split='validation', transform=transform_val)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/Development/Interp/mnist-sae/.venv/lib/python3.12/site-packages/torchvision/datasets/eurosat.py:41\u001b[0m, in \u001b[0;36mEuroSAT.__init__\u001b[0;34m(self, root, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_exists():\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_folder, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(root)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dataset not found. You can use download=True to download it"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(28, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Normalization for validation\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "data_name = 'EuroSAT'\n",
    "\n",
    "# Load CIFAR10 dataset\n",
    "# train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "# val_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_val)\n",
    "\n",
    "# Load CIFAR100 dataset\n",
    "# train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "# val_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    # Undo normalization\n",
    "    mean = np.array([0.4914, 0.4822, 0.4465])\n",
    "    std = np.array([0.2023, 0.1994, 0.2010])\n",
    "    \n",
    "    # Convert to numpy and transpose from (C,H,W) to (H,W,C)\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    # Unnormalize\n",
    "    img = std * img + mean\n",
    "    \n",
    "    # Clip values to be between 0 and 1\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "# Get a random image from the training set\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# # Display a single image\n",
    "# plt.figure(figsize=(4, 4))\n",
    "# imshow(images[0])\n",
    "# plt.title(f'Class: {labels[0].item()}')\n",
    "# plt.show()\n",
    "\n",
    "# If you want to display the actual class name\n",
    "classes = train_dataset.classes  # Get class names\n",
    "plt.figure(figsize=(4, 4))\n",
    "imshow(images[0])\n",
    "plt.title(f'Class: {classes[labels[0].item()]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bl/kzfk5ts90gj98y9jcv49ynmc0000gn/T/ipykernel_74447/2310615750.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('models/mnist_colored.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from structs.models import MNISTModel, ColoredMNISTModel\n",
    "# load in trained mnist model \n",
    "model = ColoredMNISTModel()\n",
    "model.load_state_dict(torch.load('models/mnist_colored.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_layer_config = fc1_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:15<00:00, 50.65it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.clear_cache()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        outputs = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the train_loader: 50000\n"
     ]
    }
   ],
   "source": [
    "num_images = len(train_loader.dataset)\n",
    "print(f'Number of images in the train_loader: {num_images}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 256])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_activations = model.get_cached_activations(selected_layer_config.name)\n",
    "analysis_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(analysis_activations, f'embeddings/mnist_{selected_layer_config.name}_{data_name}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sae Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bl/kzfk5ts90gj98y9jcv49ynmc0000gn/T/ipykernel_74447/2399077495.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sae.load_state_dict(torch.load('models/mnist_sae_colored.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from structs.models import SimpleSAE, EnhancedSAE\n",
    "\n",
    "input_dim = selected_layer_config.input_dim\n",
    "hidden_dim = 2304\n",
    "sae = EnhancedSAE(input_dim=input_dim, hidden_dim=hidden_dim, l1_coeff=0.01)\n",
    "sae.load_state_dict(torch.load('models/mnist_sae_colored.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bl/kzfk5ts90gj98y9jcv49ynmc0000gn/T/ipykernel_74447/1027644296.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  analysis_activations = torch.load(f'embeddings/mnist_{fc1_config.name}_{data_name}.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 256])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_activations = torch.load(f'embeddings/mnist_{fc1_config.name}_{data_name}.pth')\n",
    "analysis_loader = DataLoader(analysis_activations, batch_size=batch_size, shuffle=False) # do not shuffle\n",
    "\n",
    "analysis_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:00<00:00, 2064.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# pass the activations through the SAE and save the intermediary activations \n",
    "sae.clear_cache()\n",
    "sae.eval()\n",
    "with torch.no_grad():\n",
    "    for activations in tqdm(analysis_loader):\n",
    "        encoded, decoded = sae(activations, cache_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 2304])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae_activations = sae.get_cached_activations('encoder')\n",
    "torch.save(sae_activations, f'embeddings/mnist_encoder_{data_name}.pth')\n",
    "sae_activations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bl/kzfk5ts90gj98y9jcv49ynmc0000gn/T/ipykernel_74447/2414796192.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sae_activations = torch.load(f'embeddings/mnist_encoder_{data_name}.pth')\n"
     ]
    }
   ],
   "source": [
    "sae_activations = torch.load(f'embeddings/mnist_encoder_{data_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2304, 50000])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transpose is to make it by neuron as opposed to by sample \n",
    "transposed = sae_activations.T\n",
    "transposed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_10_indices_per_neuron = torch.argsort(transposed, descending=True, dim=1)[:, :10]\n",
    "max_10_indices_per_neuron_value = torch.gather(transposed, 1, max_10_indices_per_neuron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "original_dataset = datasets.EuroSAT(root='./data', train=True, download=True, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_indices_save(indices, filename, neuron_idx):\n",
    "    plt.figure(figsize=(15, 4))  # Adjusted figure size for 10 images\n",
    "    count = 0\n",
    "    for i, idx in enumerate(indices):\n",
    "        if transposed[neuron_idx][idx] == 0:\n",
    "            # print('skipped idx', idx)\n",
    "            continue\n",
    "        count += 1\n",
    "\n",
    "\n",
    "        img = train_dataset.data[idx]\n",
    "        \n",
    "        plt.subplot(2, 5, i+1)  # 2 rows, 5 columns\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Index: {idx}')\n",
    "\n",
    "    if plt.gca().has_data():\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'docs/neuron_{data_name}/{count}_{filename}')  # Save the figure to a file\n",
    "    plt.close()  # Close the figure to free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Plotting neurons: 100%|██████████| 2304/2304 [00:40<00:00, 57.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os \n",
    "\n",
    "os.makedirs(f'docs/neuron_{data_name}', exist_ok=True)\n",
    "\n",
    "for i in tqdm(range(len(max_10_indices_per_neuron)), desc=\"Plotting neurons\"):\n",
    "    indices = max_10_indices_per_neuron[i]\n",
    "    plot_indices_save(indices, f'neuron_{i}_plots.png', i)  # Save individual plots if needed\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
